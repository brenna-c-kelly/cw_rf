{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulations\n",
    "\n",
    "This file includes the code for generating the simulated data. Each data file will contain a set of 5000 exposures randomly sampled without replacement from the Utah vital records cohort, the synthetic confounder, and the synthetic outcome. Indicator variables are also included to describe the critical window's structure.  \n",
    "\n",
    "This runs 600 simulations for interactions within a moderate, smooth critical window in the middle of the 20-week period."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Outside of loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "import multiprocessing as mp\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats import norm\n",
    "import matplotlib.pyplot as plt\n",
    "from econml.dml import LinearDML\n",
    "from econml.dml import CausalForestDML\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Critical window coefficient creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "study_period = np.linspace(1, 20, 20)\n",
    "effect_size = -15 # total reduction of 15 percentiles (based on the average effect of a 200-gram reduction in birthweight, given Sun et al)\n",
    "# the window should be 9 wide, then 7 wide, then 3 wide\n",
    "window_start = 6\n",
    "window_center = 10\n",
    "window_end = 14\n",
    "\n",
    "uni_pdf = stats.Uniform(a = window_start, b = window_end)\n",
    "uni_wide_fx  = uni_pdf.pdf(study_period) * effect_size\n",
    "\n",
    "uni_pdf = stats.Uniform(a = window_start + 1, b = window_end - 1)\n",
    "uni_moderate_fx  = uni_pdf.pdf(study_period) * effect_size\n",
    "\n",
    "uni_pdf = stats.Uniform(a = window_start + 3, b = window_end - 3)\n",
    "uni_narrow_fx  = uni_pdf.pdf(study_period) * effect_size\n",
    "\n",
    "norm_pdf = stats.norm.pdf(study_period, window_center, 2.5)\n",
    "norm_wide_fx  = norm_pdf * effect_size \n",
    "\n",
    "norm_pdf = stats.norm.pdf(study_period, window_center, 1)\n",
    "norm_moderate_fx  = norm_pdf * effect_size \n",
    "\n",
    "norm_pdf = stats.norm.pdf(study_period, window_center, 0.5)\n",
    "norm_narrow_fx = norm_pdf * effect_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-12.017118842260802"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(norm_moderate_fx[8] + norm_moderate_fx[9]) * 1.25\n",
    "\n",
    "# intx_effect = (norm_moderate_fx[8] + norm_moderate_fx[9]) * 1.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combinations to be simulated\n",
    "cw_coefs = pd.DataFrame({#\"norm_wide_fx\": norm_wide_fx, \n",
    "              \"norm_moderate_fx\": norm_moderate_fx})#, \n",
    "              #\"norm_narrow_fx\": norm_narrow_fx,\n",
    "              #\"uni_wide_fx\": uni_wide_fx, \n",
    "              #\"uni_moderate_fx\": uni_moderate_fx, \n",
    "              #\"uni_narrow_fx\": uni_narrow_fx})\n",
    "\n",
    "cw_combos = pd.DataFrame({\"coefficients\": [cw_coefs[\"norm_moderate_fx\"]],#, \n",
    "                                           #cw_coefs[\"norm_narrow_fx\"],\n",
    "                                           #cw_coefs[\"uni_wide_fx\"], \n",
    "                                           #cw_coefs[\"uni_moderate_fx\"], \n",
    "                                           #cw_coefs[\"uni_narrow_fx\"]],\n",
    "                          \"sizes\": [#\"wide\", \n",
    "                                    \"moderate\"],\n",
    "                                    #, \"narrow\",\n",
    "                                    #\"wide\", \"moderate\", \"narrow\"],\n",
    "                          \"times\": [#\"smooth\", \n",
    "                                    \"smooth\"]})\n",
    "                                    #, \"smooth\",\n",
    "                                    #\"naive\", \"naive\", \"naive\"]})\n",
    "\n",
    "# save, for plotting\n",
    "cw_coefs.to_csv(\"data/\" + \"true_intx_cw_fx_\" + str(effect_size) + \".csv\", sep = ',', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Reading in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/75/b6cym0892z93rs6dbfy789240000gn/T/ipykernel_445/2057111297.py:1: DtypeWarning: Columns (15,23) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  births = pd.read_csv(\"data/birth_w_percentile_confounders.csv\")\n"
     ]
    }
   ],
   "source": [
    "births = pd.read_csv(\"data/birth_w_percentile_confounders.csv\")\n",
    "births.head()\n",
    "\n",
    "o3_mean = births[['max_o3_01', 'max_o3_02', 'max_o3_03', 'max_o3_04', \n",
    "                  'max_o3_05', 'max_o3_06', 'max_o3_07', 'max_o3_08', \n",
    "                  'max_o3_09', 'max_o3_10', 'max_o3_11', 'max_o3_12', \n",
    "                  'max_o3_13', 'max_o3_14', 'max_o3_15', 'max_o3_16', \n",
    "                  'max_o3_17', 'max_o3_18', 'max_o3_19', 'max_o3_20']].values.mean()\n",
    "o3_sd = births[['max_o3_01', 'max_o3_02', 'max_o3_03', 'max_o3_04', \n",
    "                'max_o3_05', 'max_o3_06', 'max_o3_07', 'max_o3_08', \n",
    "                'max_o3_09', 'max_o3_10', 'max_o3_11', 'max_o3_12', \n",
    "                'max_o3_13', 'max_o3_14', 'max_o3_15', 'max_o3_16', \n",
    "                'max_o3_17', 'max_o3_18', 'max_o3_19', 'max_o3_20']].values.std()\n",
    "\n",
    "bwp_mean = births[\"bw_percentile\"].mean()\n",
    "bwp_sd = births[\"bw_percentile\"].std()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def sim_function(iteration, cw_window_type, window_size, window_time):\n",
    "        \n",
    "#     ## collecting exposure data\n",
    "\n",
    "#     n_samples = 5000\n",
    "\n",
    "#     n_X = 1\n",
    "#     n_T = 20\n",
    "#     n_W = 1\n",
    "\n",
    "#     # treatments / exposures; 10 ppb scale\n",
    "#     T_sample = births.sample(n = n_samples, replace = False)\n",
    "\n",
    "#     T_01 = (T_sample['max_o3_01'] - o3_mean) / 10\n",
    "#     T_03 = (T_sample['max_o3_03'] - o3_mean) / 10\n",
    "#     T_02 = (T_sample['max_o3_02'] - o3_mean) / 10\n",
    "#     T_04 = (T_sample['max_o3_04'] - o3_mean) / 10\n",
    "#     T_05 = (T_sample['max_o3_05'] - o3_mean) / 10\n",
    "#     T_06 = (T_sample['max_o3_06'] - o3_mean) / 10\n",
    "#     T_07 = (T_sample['max_o3_07'] - o3_mean) / 10\n",
    "#     T_08 = (T_sample['max_o3_08'] - o3_mean) / 10\n",
    "#     T_09 = (T_sample['max_o3_09'] - o3_mean) / 10\n",
    "#     T_10 = (T_sample['max_o3_10'] - o3_mean) / 10\n",
    "#     T_11 = (T_sample['max_o3_11'] - o3_mean) / 10\n",
    "#     T_12 = (T_sample['max_o3_12'] - o3_mean) / 10\n",
    "#     T_13 = (T_sample['max_o3_13'] - o3_mean) / 10\n",
    "#     T_14 = (T_sample['max_o3_14'] - o3_mean) / 10\n",
    "#     T_15 = (T_sample['max_o3_15'] - o3_mean) / 10\n",
    "#     T_16 = (T_sample['max_o3_16'] - o3_mean) / 10\n",
    "#     T_17 = (T_sample['max_o3_17'] - o3_mean) / 10\n",
    "#     T_18 = (T_sample['max_o3_18'] - o3_mean) / 10\n",
    "#     T_19 = (T_sample['max_o3_19'] - o3_mean) / 10\n",
    "#     T_20 = (T_sample['max_o3_20'] - o3_mean) / 10\n",
    "\n",
    "#     # stack\n",
    "#     T_vars = np.vstack((T_01, T_02, T_03, T_04, T_05,\n",
    "#                         T_06, T_07, T_08, T_09, T_10,\n",
    "#                         T_11, T_12, T_13, T_14, T_15,\n",
    "#                         T_16, T_17, T_18, T_19, T_20))\n",
    "\n",
    "#     ## creating confounder coefficient\n",
    "\n",
    "#     # we're only confounding the critical exposures if the effect is < -5 (can be changed later)\n",
    "#     critical_to_confound = [index for index, value in enumerate(cw_window_type) if value < -0.5]\n",
    "\n",
    "#     X = [sum(fx) for fx in zip(*T_vars[critical_to_confound]*1.25)] + np.random.normal(size = n_samples, loc = 0, scale = 1)\n",
    "#     # z scale\n",
    "#     X = X / X.std()\n",
    "#     X = pd.Series(X)\n",
    "#     X = X.to_numpy()\n",
    "\n",
    "#     ## creating the outcome\n",
    "\n",
    "#     # the coefficients and treatment variables\n",
    "#     tx_fx, tx = cw_window_type, T_vars\n",
    "#     # the vectors of each coefficient * treatment variable\n",
    "#     tx_fx_list = [tx[i] * tx_fx[i] for i in np.arange(0, 20)]\n",
    "#     # tx_fx_list = [tx[10] * tx_fx[10]]\n",
    "#     # total treatment effect by individual\n",
    "#     total_tx_fx = [sum(fx) for fx in zip(*tx_fx_list)]\n",
    "\n",
    "#     # the confounder effect (z-scaled, see above)\n",
    "#     b_W0y = 1.5\n",
    "#     # the vector of confounder effects\n",
    "#     wx_fx = X * b_W0y\n",
    "\n",
    "#     # bit of noise\n",
    "#     e = np.random.normal(size=n_samples, loc = 0, scale = 1)\n",
    "\n",
    "#     b_int = bwp_mean\n",
    "\n",
    "#     # interaction #######\n",
    "#     b_intx = -12 #(norm_moderate_fx[8] + norm_moderate_fx[9]) * 1.25\n",
    "#     intx_fx = b_intx * tx[8] * tx[9] #[b_intx * tx[i] for i in np.arange(8, 10)]\n",
    "\n",
    "#     y = b_int + total_tx_fx + wx_fx + e + intx_fx\n",
    "\n",
    "#     # adding an indicator for critical / not\n",
    "#     # critical_ones = [index for index, value in enumerate(cw_window_type) if value < -10]\n",
    "\n",
    "#     # critical_or_not = []\n",
    "\n",
    "#     # for i in range(0, 20):\n",
    "#     #     if i in critical_ones:\n",
    "#     #         critical_or_not.append(\"critical\")\n",
    "#     #     else:\n",
    "#     #         critical_or_not.append(\"not critical\")\n",
    "\n",
    "#     ## store the data file\n",
    "\n",
    "#     sim_dat = pd.DataFrame({'sim_index': iteration, 'cw_size': window_size,\n",
    "#                             'cw_timedep': window_time, #\"critical\": critical_or_not,\n",
    "#                             'total_fx_size': effect_size,\n",
    "#                             'y_hat': y, 'true_y': T_sample[\"birthweightgrams\"], 'x': X,\n",
    "#                             'tx_01': T_01, 'tx_02': T_02, 'tx_03': T_03, 'tx_04': T_04, \n",
    "#                             'tx_05': T_05, 'tx_06': T_06, 'tx_07': T_07, 'tx_08': T_08, \n",
    "#                             'tx_09': T_09, 'tx_10': T_10, 'tx_11': T_11, 'tx_12': T_12, \n",
    "#                             'tx_13': T_13, 'tx_14': T_14, 'tx_15': T_15, 'tx_16': T_16, \n",
    "#                             'tx_17': T_17, 'tx_18': T_18, 'tx_19': T_19, 'tx_20': T_20})\n",
    "\n",
    "#     sim_dat.to_csv(\"data/intx_sim_data/\" + \"sim\" + str(iteration).zfill(3) + \"_\" + window_size + \"_\" + window_time + \".csv\", sep = ',', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, None]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nsims = 2\n",
    "\n",
    "# ## one cw structure\n",
    "# # [sim_function(x) for x in range(1, nsims+1)]\n",
    "\n",
    "# [sim_function(x, cw_combos.loc[0, \"coefficients\"], \n",
    "#         cw_combos.loc[0, \"sizes\"], \n",
    "#         cw_combos.loc[0, \"times\"]) for x in range(1, nsims+1)]\n",
    "\n",
    "\n",
    "# # with mp.Pool(mp.cpu_count()) as p:\n",
    "# #     p.map([sim_function(x, cw_combos.loc[1, \"coefficients\"], \n",
    "# #                         cw_combos.loc[1, \"sizes\"], \n",
    "# #                         cw_combos.loc[1, \"times\"]) for x in range(1, nsims+1)])\n",
    "# # ## all cw structures\n",
    "# # for i in range(0, len(cw_combos)-1):\n",
    "# #     [sim_function(x, cw_combos.loc[i, \"coefficients\"], \n",
    "# #              cw_combos.loc[i, \"sizes\"], \n",
    "# #              cw_combos.loc[i, \"times\"]) for x in range(1, nsims+1)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### QC\n",
    "Ran LinearDML to make sure one simulation gives expected result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T = pd.DataFrame({'noncritical_tx_1': T_01, 'noncritical_tx_2': T_02, \n",
    "#                   'noncritical_tx_3': T_03, 'noncritical_tx_4': T_04, \n",
    "#                   'noncritical_tx_5': T_05, 'noncritical_tx_6': T_06, \n",
    "#                   'noncritical_tx_7': T_07, 'critical_tx_8': T_08, \n",
    "#                   'critical_tx_9': T_09, 'critical_tx_10': T_10,\n",
    "#                   'critical_tx_11': T_11, 'critical_tx_12': T_12, \n",
    "#                   'noncritical_tx_13': T_13, 'noncritical_tx_14': T_14, \n",
    "#                   'noncritical_tx_15': T_15, 'noncritical_tx_16': T_16, \n",
    "#                   'noncritical_tx_17': T_17, 'noncritical_tx_18': T_18, \n",
    "#                   'noncritical_tx_19': T_19, 'noncritical_tx_20': T_20})\n",
    "# X = pd.DataFrame(X, columns=['confounder'])\n",
    "# y = pd.DataFrame({'birthweight': y})\n",
    "\n",
    "# model_y = 'linear'\n",
    "# model_t = 'linear'\n",
    "\n",
    "# # T = pd.DataFrame({\"tx_10\": tx[10]})\n",
    "\n",
    "# est = LinearDML(model_y=model_y, model_t=model_t,\n",
    "#                 discrete_treatment=False) \n",
    "\n",
    "# est.fit(y, T=T, W=X, X=X)\n",
    "\n",
    "# est.marginal_ate_inference(T, X)\n",
    "\n",
    "# ### seems to struggle with collinearity\n",
    "# ### - sometimes adjacent time steps are significant\n",
    "# ### - sometimes far time steps are significant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying simulated data to causal RF  \n",
    "\n",
    "This bit runs the causal RF with each simulated data file and stores the results.  \n",
    "Question: should we compare the causal random forest to the LinearDML?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Note: sim number is stored in the csv, in case we want additional comparisons later\n",
    "# sim_files = os.listdir(\"data/intx_sim_data/\")\n",
    "\n",
    "# sim_csvs = []\n",
    "\n",
    "# for file_name in sim_files:\n",
    "#     if file_name.endswith('.csv'):\n",
    "#         sim_csvs.append(file_name)\n",
    "\n",
    "# sim_csvs.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dml_function(iteration, cw_window_type, window_size, window_time):\n",
    "\n",
    "    #### Part I: data generation\n",
    "    n_samples = 5000\n",
    "\n",
    "    n_X = 1\n",
    "    n_T = 20\n",
    "    n_W = 1\n",
    "\n",
    "    # treatments / exposures; 10 ppb scale\n",
    "    T_sample = births.sample(n = n_samples, replace = False)\n",
    "\n",
    "    T_01 = (T_sample['max_o3_01'] - o3_mean) / 10\n",
    "    T_03 = (T_sample['max_o3_03'] - o3_mean) / 10\n",
    "    T_02 = (T_sample['max_o3_02'] - o3_mean) / 10\n",
    "    T_04 = (T_sample['max_o3_04'] - o3_mean) / 10\n",
    "    T_05 = (T_sample['max_o3_05'] - o3_mean) / 10\n",
    "    T_06 = (T_sample['max_o3_06'] - o3_mean) / 10\n",
    "    T_07 = (T_sample['max_o3_07'] - o3_mean) / 10\n",
    "    T_08 = (T_sample['max_o3_08'] - o3_mean) / 10\n",
    "    T_09 = (T_sample['max_o3_09'] - o3_mean) / 10\n",
    "    T_10 = (T_sample['max_o3_10'] - o3_mean) / 10\n",
    "    T_11 = (T_sample['max_o3_11'] - o3_mean) / 10\n",
    "    T_12 = (T_sample['max_o3_12'] - o3_mean) / 10\n",
    "    T_13 = (T_sample['max_o3_13'] - o3_mean) / 10\n",
    "    T_14 = (T_sample['max_o3_14'] - o3_mean) / 10\n",
    "    T_15 = (T_sample['max_o3_15'] - o3_mean) / 10\n",
    "    T_16 = (T_sample['max_o3_16'] - o3_mean) / 10\n",
    "    T_17 = (T_sample['max_o3_17'] - o3_mean) / 10\n",
    "    T_18 = (T_sample['max_o3_18'] - o3_mean) / 10\n",
    "    T_19 = (T_sample['max_o3_19'] - o3_mean) / 10\n",
    "    T_20 = (T_sample['max_o3_20'] - o3_mean) / 10\n",
    "\n",
    "    # stack\n",
    "    T_vars = np.vstack((T_01, T_02, T_03, T_04, T_05,\n",
    "                        T_06, T_07, T_08, T_09, T_10,\n",
    "                        T_11, T_12, T_13, T_14, T_15,\n",
    "                        T_16, T_17, T_18, T_19, T_20))\n",
    "\n",
    "    ## creating confounder coefficient\n",
    "\n",
    "    # we're only confounding the critical exposures if the effect is < -5 (can be changed later)\n",
    "    critical_to_confound = [index for index, value in enumerate(cw_window_type) if value < -0.5]\n",
    "\n",
    "    X = [sum(fx) for fx in zip(*T_vars[critical_to_confound]*1.25)] + np.random.normal(size = n_samples, loc = 0, scale = 1)\n",
    "    # z scale\n",
    "    X = X / X.std()\n",
    "    X = pd.Series(X)\n",
    "    X = X.to_numpy()\n",
    "\n",
    "    ## creating the outcome\n",
    "\n",
    "    # the coefficients and treatment variables\n",
    "    tx_fx, tx = cw_window_type, T_vars\n",
    "    # the vectors of each coefficient * treatment variable\n",
    "    tx_fx_list = [tx[i] * tx_fx[i] for i in np.arange(0, 20)]\n",
    "    # tx_fx_list = [tx[10] * tx_fx[10]]\n",
    "    # total treatment effect by individual\n",
    "    total_tx_fx = [sum(fx) for fx in zip(*tx_fx_list)]\n",
    "\n",
    "    # the confounder effect (z-scaled, see above)\n",
    "    b_W0y = 1.5\n",
    "    # the vector of confounder effects\n",
    "    wx_fx = X * b_W0y\n",
    "\n",
    "    # bit of noise\n",
    "    e = np.random.normal(size=n_samples, loc = 0, scale = 1)\n",
    "\n",
    "    # interaction\n",
    "    b_intx = -12\n",
    "    intx_fx = b_intx * tx[8] * tx[9] #[b_intx * tx[i] for i in np.arange(8, 10)]\n",
    "\n",
    "    b_int = bwp_mean\n",
    "\n",
    "    y = b_int + total_tx_fx + wx_fx + e + intx_fx\n",
    "\n",
    "    sim_dat = pd.DataFrame({'sim_index': iteration, 'cw_size': window_size,\n",
    "                            'cw_timedep': window_time, #\"critical\": critical_or_not,\n",
    "                            'total_fx_size': effect_size,\n",
    "                            'y_hat': y, 'true_y': T_sample[\"birthweightgrams\"], 'x': X,\n",
    "                            'tx_01': T_01, 'tx_02': T_02, 'tx_03': T_03, 'tx_04': T_04, \n",
    "                            'tx_05': T_05, 'tx_06': T_06, 'tx_07': T_07, 'tx_08': T_08, \n",
    "                            'tx_09': T_09, 'tx_10': T_10, 'tx_11': T_11, 'tx_12': T_12, \n",
    "                            'tx_13': T_13, 'tx_14': T_14, 'tx_15': T_15, 'tx_16': T_16, \n",
    "                            'tx_17': T_17, 'tx_18': T_18, 'tx_19': T_19, 'tx_20': T_20,\n",
    "                            'tx_9_10': T_09*T_10})\n",
    "\n",
    "\n",
    "    #### Part II: model training\n",
    "    # dir = \"data/sims/\" + \"sim\" + str(iteration).zfill(3) + \"_\" + window_size + \"_\" + window_time + \".csv\"\n",
    "    # sim_dat = pd.read_csv(dir)\n",
    "\n",
    "    T = sim_dat.loc[:, \"tx_01\":\"tx_9_10\"]\n",
    "    X = pd.DataFrame({\"confounder\": sim_dat[\"x\"]})\n",
    "    y = pd.DataFrame({\"birthweight\": sim_dat[\"y_hat\"]})\n",
    "    # convert to 1d array\n",
    "    # y = y.to_numpy().flatten()\n",
    "\n",
    "    ## for interactions\n",
    "\n",
    "    # featurizer = PolynomialFeatures(degree = 2, interaction_only = True, include_bias = False, order = 'F')\n",
    "\n",
    "    # T_interactions = featurizer.fit_transform(T)\n",
    "\n",
    "    print(\"test0\")\n",
    "\n",
    "    est = CausalForestDML(model_t='forest',\n",
    "                          model_y='forest',\n",
    "                          discrete_treatment=False,\n",
    "                          n_estimators=500,\n",
    "                          n_jobs = 4)\n",
    "\n",
    "    est.fit(y, T=T, X=X, W=X)\n",
    "\n",
    "    print(\"test1\")\n",
    "\n",
    "    # # extract results\n",
    "    treatments = np.array(est.cate_treatment_names())\n",
    "    res = est.marginal_ate_inference(T, X)\n",
    "    means = res.mean_point #est.marginal_ate(T, X)\n",
    "    ci_lower, ci_upper = res.conf_int_mean()\n",
    "    p_vals = res.pvalue()\n",
    "\n",
    "    # print(res)\n",
    "    # print(len(res))\n",
    "\n",
    "    # # create dataframe\n",
    "    res_df = pd.DataFrame({\n",
    "        # 'sim_index': iteration,\n",
    "        # 'cw_size': window_size, 'cw_timedep': window_time,\n",
    "        'treatment': treatments,\n",
    "        # 'true_effect': cw_window_type,\n",
    "        'mean': means[0],\n",
    "        'ci_lower': ci_lower[0],\n",
    "        'ci_upper': ci_upper[0],\n",
    "        'p_value': p_vals[0]\n",
    "    })\n",
    "    \n",
    "    # indicate if critical\n",
    "    # res_df[\"cw\"] = [\"critical\" if x < -0.5 else \"not critical\" for x in res_df[\"true_effect\"]]\n",
    "\n",
    "    ## test results for critical window\n",
    "\n",
    "    # 1. is the true effect recovered for each week?\n",
    "    # res_df[\"effect_recovered\"] = res_df[\"true_effect\"].between(res_df[\"ci_lower\"], res_df[\"ci_upper\"])\n",
    "\n",
    "    # 2. did it get the trend? i.e., the difference in value over time\n",
    "    # what the difference should be:\n",
    "    # diff_list_true = [np.nan]\n",
    "    # for i in range(1, len(cw_window_type)):\n",
    "    #     diff_list_true.append(cw_window_type[i] - cw_window_type[i - 1])\n",
    "    # res_df[\"trend_true\"] = diff_list_true\n",
    "\n",
    "    # diff_list_pred = [np.nan]\n",
    "    # for i in range(1, len(res_df[\"mean\"])):\n",
    "    #     diff_list_pred.append(res_df.loc[i, \"mean\"] - res_df.loc[i - 1, \"mean\"])\n",
    "    # res_df[\"trend_pred\"] = diff_list_pred\n",
    "\n",
    "    # # how close is the true trend to the observed trend?\n",
    "    # res_df[\"trend_recovered_diff\"] = (res_df[\"trend_true\"] - res_df[\"trend_pred\"])\n",
    "\n",
    "    # # 3. did it get the peak effect? i.e., the inflection point\n",
    "    # # what the inflection point should be:\n",
    "    # inflection = []\n",
    "    # for i in range(len(diff_list_true) - 1):\n",
    "    #     if diff_list_true[i] < 0 and diff_list_true[i + 1] > 0:\n",
    "    #         inflection.append(\"peak\")\n",
    "    #     else:\n",
    "    #         inflection.append(\"not peak\")\n",
    "    # inflection.append(np.nan) # no next value\n",
    "    # res_df[\"inflection_true\"] = inflection\n",
    "\n",
    "    # # what was the predicted inflection point?\n",
    "    # inflection = []\n",
    "    # for i in range(len(diff_list_pred) - 1):\n",
    "    #     if diff_list_pred[i] < 0 and diff_list_pred[i + 1] > 0:\n",
    "    #         inflection.append(\"peak\")\n",
    "    #     else:\n",
    "    #         inflection.append(\"not peak\")\n",
    "    # inflection.append(np.nan) # no next value\n",
    "    # res_df[\"inflection_pred\"] = inflection\n",
    "\n",
    "    # # is the peak effect recovered?\n",
    "    # res_df[\"inflection_recovered\"] = (res_df[\"inflection_true\"] == res_df[\"inflection_pred\"])\n",
    "\n",
    "    # # 4. was the critical effect (>5) statistically significant at alpha = 0.05?\n",
    "    # critical_sig = []\n",
    "\n",
    "    # for index, item in enumerate(res_df[\"p_value\"]):\n",
    "    #     if index in [index for index, value in enumerate(cw_window_type) if value < -0.5]:\n",
    "    #         if item < 0.05:\n",
    "    #             critical_sig.append(\"TRUE\")\n",
    "    #         else:\n",
    "    #             critical_sig.append(\"FALSE\")\n",
    "    #     else:\n",
    "    #         critical_sig.append(np.nan)\n",
    "\n",
    "    # res_df[\"critical_sig_recovered\"] = critical_sig\n",
    "\n",
    "    res_df.to_csv(\"data/intx_sim_results/\" + \"sim\" + str(iteration).zfill(3) + \"_res_intx\" + \".csv\", sep = ',', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/base.py:1389: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/base.py:1389: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/base.py:1389: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/base.py:1389: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/base.py:1389: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<econml.dml.causal_forest.CausalForestDML at 0x12edc5eb0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# n_samples = 5000\n",
    "\n",
    "# n_X = 1\n",
    "# n_T = 20\n",
    "# n_W = 1\n",
    "\n",
    "#     # treatments / exposures; 10 ppb scale\n",
    "# T_sample = births.sample(n = n_samples, replace = False)\n",
    "\n",
    "# T_01 = (T_sample['max_o3_01'] - o3_mean) / 10\n",
    "# T_03 = (T_sample['max_o3_03'] - o3_mean) / 10\n",
    "# T_02 = (T_sample['max_o3_02'] - o3_mean) / 10\n",
    "# T_04 = (T_sample['max_o3_04'] - o3_mean) / 10\n",
    "# T_05 = (T_sample['max_o3_05'] - o3_mean) / 10\n",
    "# T_06 = (T_sample['max_o3_06'] - o3_mean) / 10\n",
    "# T_07 = (T_sample['max_o3_07'] - o3_mean) / 10\n",
    "# T_08 = (T_sample['max_o3_08'] - o3_mean) / 10\n",
    "# T_09 = (T_sample['max_o3_09'] - o3_mean) / 10\n",
    "# T_10 = (T_sample['max_o3_10'] - o3_mean) / 10\n",
    "# T_11 = (T_sample['max_o3_11'] - o3_mean) / 10\n",
    "# T_12 = (T_sample['max_o3_12'] - o3_mean) / 10\n",
    "# T_13 = (T_sample['max_o3_13'] - o3_mean) / 10\n",
    "# T_14 = (T_sample['max_o3_14'] - o3_mean) / 10\n",
    "# T_15 = (T_sample['max_o3_15'] - o3_mean) / 10\n",
    "# T_16 = (T_sample['max_o3_16'] - o3_mean) / 10\n",
    "# T_17 = (T_sample['max_o3_17'] - o3_mean) / 10\n",
    "# T_18 = (T_sample['max_o3_18'] - o3_mean) / 10\n",
    "# T_19 = (T_sample['max_o3_19'] - o3_mean) / 10\n",
    "# T_20 = (T_sample['max_o3_20'] - o3_mean) / 10\n",
    "\n",
    "#     # stack\n",
    "# T_vars = np.vstack((T_01, T_02, T_03, T_04, T_05,\n",
    "#                     T_06, T_07, T_08, T_09, T_10,\n",
    "#                     T_11, T_12, T_13, T_14, T_15,\n",
    "#                     T_16, T_17, T_18, T_19, T_20))\n",
    "\n",
    "#     ## creating confounder coefficient\n",
    "\n",
    "#     # we're only confounding the critical exposures if the effect is < -5 (can be changed later)\n",
    "# critical_to_confound = [index for index, value in enumerate(norm_moderate_fx) if value < -0.5]\n",
    "\n",
    "# X = [sum(fx) for fx in zip(*T_vars[critical_to_confound]*1.25)] + np.random.normal(size = n_samples, loc = 0, scale = 1)\n",
    "#     # z scale\n",
    "# X = X / X.std()\n",
    "# X = pd.Series(X)\n",
    "# X = X.to_numpy()\n",
    "\n",
    "#     ## creating the outcome\n",
    "\n",
    "#     # the coefficients and treatment variables\n",
    "# tx_fx, tx = norm_moderate_fx, T_vars\n",
    "#     # the vectors of each coefficient * treatment variable\n",
    "# tx_fx_list = [tx[i] * tx_fx[i] for i in np.arange(0, 20)]\n",
    "#     # tx_fx_list = [tx[10] * tx_fx[10]]\n",
    "#     # total treatment effect by individual\n",
    "# total_tx_fx = [sum(fx) for fx in zip(*tx_fx_list)]\n",
    "\n",
    "#     # the confounder effect (z-scaled, see above)\n",
    "# b_W0y = 1.5\n",
    "#     # the vector of confounder effects\n",
    "# wx_fx = X * b_W0y\n",
    "\n",
    "#     # bit of noise\n",
    "# e = np.random.normal(size=n_samples, loc = 0, scale = 1)\n",
    "\n",
    "#     # interaction\n",
    "# b_intx = -12\n",
    "# intx_fx = b_intx * tx[8] * tx[9] #[b_intx * tx[i] for i in np.arange(8, 10)]\n",
    "\n",
    "# b_int = bwp_mean\n",
    "\n",
    "# y = b_int + total_tx_fx + wx_fx + e + intx_fx\n",
    "\n",
    "# sim_dat = pd.DataFrame({'sim_index': 1,# 'cw_size': window_size,\n",
    "#                         #'cw_timedep': window_time, #\"critical\": critical_or_not,\n",
    "#                         'total_fx_size': effect_size,\n",
    "#                         'y_hat': y, 'true_y': T_sample[\"birthweightgrams\"], 'x': X,\n",
    "#                         'tx_01': T_01, 'tx_02': T_02, 'tx_03': T_03, 'tx_04': T_04, \n",
    "#                         'tx_05': T_05, 'tx_06': T_06, 'tx_07': T_07, 'tx_08': T_08, \n",
    "#                         'tx_09': T_09, 'tx_10': T_10, 'tx_11': T_11, 'tx_12': T_12, \n",
    "#                         'tx_13': T_13, 'tx_14': T_14, 'tx_15': T_15, 'tx_16': T_16, \n",
    "#                         'tx_17': T_17, 'tx_18': T_18, 'tx_19': T_19, 'tx_20': T_20,\n",
    "#                         'tx_9_10': T_09*T_10})\n",
    "\n",
    "\n",
    "#     #### Part II: model training\n",
    "#     # dir = \"data/sims/\" + \"sim\" + str(iteration).zfill(3) + \"_\" + window_size + \"_\" + window_time + \".csv\"\n",
    "#     # sim_dat = pd.read_csv(dir)\n",
    "\n",
    "# T = sim_dat.loc[:, \"tx_01\":\"tx_9_10\"]\n",
    "# X = pd.DataFrame({\"confounder\": sim_dat[\"x\"]})\n",
    "# y = pd.DataFrame({\"birthweight\": sim_dat[\"y_hat\"]})\n",
    "\n",
    "# est = CausalForestDML(model_t='forest',\n",
    "#                           model_y='forest',\n",
    "#                           discrete_treatment=False,\n",
    "#                           n_estimators=500,\n",
    "#                           n_jobs = 4)\n",
    "\n",
    "# est.fit(y, T=T, X=X, W=X)\n",
    "\n",
    "\n",
    "# res_df = pd.DataFrame({\n",
    "#     'sim_index': iteration,\n",
    "#     # 'cw_size': window_size, 'cw_timedep': window_time,\n",
    "#     'treatment': treatments,\n",
    "#     'true_effect': cw_window_type,\n",
    "#     'mean': means[0],\n",
    "#     'ci_lower': ci_lower[0],\n",
    "#     'ci_upper': ci_upper[0],\n",
    "#     'p_value': p_vals[0]\n",
    "# })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# treatments = np.array(est.cate_treatment_names())\n",
    "# res = est.marginal_ate_inference(T, X)\n",
    "# means = res.mean_point #est.marginal_ate(T, X)\n",
    "# ci_lower, ci_upper = res.conf_int_mean()\n",
    "# p_vals = res.pvalue()\n",
    "\n",
    "# res_df = pd.DataFrame({\n",
    "#     'sim_index': 1,\n",
    "#     # 'cw_size': window_size, 'cw_timedep': window_time,\n",
    "#     'treatment': treatments,\n",
    "#     # 'true_effect': cw_window_type,\n",
    "#     'mean': means[0],\n",
    "#     'ci_lower': ci_lower[0],\n",
    "#     'ci_upper': ci_upper[0],\n",
    "#     'p_value': p_vals[0]\n",
    "# })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/base.py:1389: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/base.py:1389: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/base.py:1389: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/base.py:1389: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/base.py:1389: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test1\n",
      "   treatment       mean   ci_lower  ci_upper       p_value\n",
      "0      tx_01  -0.022644  -1.256859  1.211570  9.713146e-01\n",
      "1      tx_02  -0.046218  -1.110740  1.018304  9.321859e-01\n",
      "2      tx_03   0.013331  -1.069396  1.096058  9.807476e-01\n",
      "3      tx_04  -0.037077  -1.045732  0.971578  9.425655e-01\n",
      "4      tx_05  -0.066816  -1.049222  0.915591  8.939551e-01\n",
      "5      tx_06   0.152464  -0.838360  1.143287  7.629639e-01\n",
      "6      tx_07  -0.158991  -1.197739  0.879758  7.641831e-01\n",
      "7      tx_08  -0.791746  -1.924568  0.341077  1.707347e-01\n",
      "8      tx_09  -3.539834  -6.279046 -0.800623  1.131469e-02\n",
      "9      tx_10  -6.018129  -8.627134 -3.409124  6.154790e-06\n",
      "10     tx_11  -3.715517  -4.806426 -2.624608  2.465201e-11\n",
      "11     tx_12  -0.711815  -1.789397  0.365766  1.954278e-01\n",
      "12     tx_13  -0.071172  -1.020523  0.878178  8.831815e-01\n",
      "13     tx_14  -0.186592  -1.194161  0.820978  7.166307e-01\n",
      "14     tx_15   0.097126  -0.971742  1.165994  8.586460e-01\n",
      "15     tx_16  -0.002838  -1.036344  1.030667  9.957051e-01\n",
      "16     tx_17   0.016531  -0.996335  1.029398  9.744807e-01\n",
      "17     tx_18   0.063334  -0.923461  1.050128  8.998958e-01\n",
      "18     tx_19  -0.015362  -1.257630  1.226906  9.806639e-01\n",
      "19     tx_20  -0.103320  -1.196573  0.989933  8.530488e-01\n",
      "20   tx_9_10 -12.061424 -14.502289 -9.620558  3.489707e-22\n",
      "test0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/base.py:1389: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/base.py:1389: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/base.py:1389: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/base.py:1389: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/base.py:1389: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test1\n",
      "   treatment       mean   ci_lower  ci_upper       p_value\n",
      "0      tx_01  -0.004875  -1.072417  1.062667  9.928584e-01\n",
      "1      tx_02   0.023726  -0.951933  0.999385  9.619851e-01\n",
      "2      tx_03  -0.001481  -0.918259  0.915297  9.974741e-01\n",
      "3      tx_04   0.007839  -0.925327  0.941004  9.868643e-01\n",
      "4      tx_05  -0.072913  -1.023618  0.877791  8.805144e-01\n",
      "5      tx_06   0.126790  -0.832182  1.085761  7.955306e-01\n",
      "6      tx_07  -0.124524  -1.163220  0.914173  8.142323e-01\n",
      "7      tx_08  -0.813106  -1.854326  0.228114  1.258762e-01\n",
      "8      tx_09  -3.706061  -6.192283 -1.219839  3.482388e-03\n",
      "9      tx_10  -6.300510  -8.883967 -3.717053  1.753469e-06\n",
      "10     tx_11  -3.653025  -4.741730 -2.564319  4.818703e-11\n",
      "11     tx_12  -0.777021  -1.820647  0.266605  1.444905e-01\n",
      "12     tx_13   0.022563  -0.955207  1.000334  9.639250e-01\n",
      "13     tx_14  -0.017249  -0.916045  0.881547  9.699952e-01\n",
      "14     tx_15  -0.120834  -1.057159  0.815492  8.003184e-01\n",
      "15     tx_16  -0.036127  -0.975645  0.903392  9.399244e-01\n",
      "16     tx_17   0.038240  -0.975158  1.051638  9.410435e-01\n",
      "17     tx_18   0.088540  -0.883888  1.060968  8.583655e-01\n",
      "18     tx_19  -0.058002  -1.093574  0.977570  9.125861e-01\n",
      "19     tx_20   0.020150  -0.992420  1.032719  9.688887e-01\n",
      "20   tx_9_10 -12.052043 -14.515241 -9.588844  8.825968e-22\n",
      "test0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/base.py:1389: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/base.py:1389: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/base.py:1389: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/base.py:1389: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/base.py:1389: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test1\n",
      "   treatment       mean   ci_lower  ci_upper       p_value\n",
      "0      tx_01   0.036602  -1.151579  1.224783  9.518554e-01\n",
      "1      tx_02  -0.150500  -1.278011  0.977012  7.936184e-01\n",
      "2      tx_03   0.155935  -0.933291  1.245162  7.790241e-01\n",
      "3      tx_04   0.091518  -0.854473  1.037508  8.496132e-01\n",
      "4      tx_05  -0.042255  -1.140699  1.056189  9.398995e-01\n",
      "5      tx_06  -0.045578  -1.022409  0.931253  9.271353e-01\n",
      "6      tx_07   0.011858  -1.001685  1.025401  9.817049e-01\n",
      "7      tx_08  -0.809434  -1.947601  0.328733  1.633558e-01\n",
      "8      tx_09  -3.553422  -6.550966 -0.555879  2.015612e-02\n",
      "9      tx_10  -5.862177  -8.846753 -2.877602  1.182730e-04\n",
      "10     tx_11  -3.545429  -4.706179 -2.384679  2.143119e-09\n",
      "11     tx_12  -0.907186  -1.977932  0.163559  9.679905e-02\n",
      "12     tx_13  -0.145077  -1.113902  0.823747  7.691428e-01\n",
      "13     tx_14   0.166996  -0.812559  1.146552  7.382752e-01\n",
      "14     tx_15  -0.064886  -1.168670  1.038899  9.082741e-01\n",
      "15     tx_16   0.081887  -0.917236  1.081011  8.723792e-01\n",
      "16     tx_17   0.007067  -0.954829  0.968963  9.885109e-01\n",
      "17     tx_18   0.029186  -1.023982  1.082353  9.566842e-01\n",
      "18     tx_19  -0.042437  -1.132360  1.047487  9.391705e-01\n",
      "19     tx_20   0.023115  -1.047059  1.093288  9.662330e-01\n",
      "20   tx_9_10 -11.888168 -14.402635 -9.373700  1.923090e-20\n",
      "test0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/base.py:1389: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/base.py:1389: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/base.py:1389: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/base.py:1389: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/base.py:1389: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test1\n",
      "   treatment       mean   ci_lower  ci_upper       p_value\n",
      "0      tx_01   0.103714  -1.072196  1.279624  8.627559e-01\n",
      "1      tx_02  -0.015798  -1.133605  1.102009  9.779011e-01\n",
      "2      tx_03   0.032896  -0.961018  1.026810  9.482779e-01\n",
      "3      tx_04  -0.024889  -0.985618  0.935841  9.595051e-01\n",
      "4      tx_05   0.041837  -0.929483  1.013156  9.327226e-01\n",
      "5      tx_06  -0.009453  -0.997911  0.979004  9.850446e-01\n",
      "6      tx_07  -0.098138  -1.098130  0.901853  8.474691e-01\n",
      "7      tx_08  -0.846511  -1.936004  0.242982  1.277972e-01\n",
      "8      tx_09  -3.357359  -6.487404 -0.227315  3.552671e-02\n",
      "9      tx_10  -5.565561  -8.691304 -2.439818  4.833345e-04\n",
      "10     tx_11  -3.665525  -4.776876 -2.554174  1.016531e-10\n",
      "11     tx_12  -0.800973  -1.893700  0.291755  1.508147e-01\n",
      "12     tx_13  -0.162534  -1.126431  0.801362  7.410272e-01\n",
      "13     tx_14   0.111442  -0.862015  1.084898  8.224636e-01\n",
      "14     tx_15  -0.034885  -1.066320  0.996550  9.471474e-01\n",
      "15     tx_16  -0.154036  -1.165868  0.857797  7.654178e-01\n",
      "16     tx_17   0.036095  -0.991088  1.063278  9.450914e-01\n",
      "17     tx_18   0.008344  -1.086752  1.103440  9.880847e-01\n",
      "18     tx_19   0.094162  -1.055242  1.243565  8.724362e-01\n",
      "19     tx_20   0.025999  -0.963337  1.015334  9.589228e-01\n",
      "20   tx_9_10 -12.108534 -14.665514 -9.551554  1.673045e-20\n",
      "test0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/base.py:1389: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/base.py:1389: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/base.py:1389: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/base.py:1389: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/base.py:1389: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test1\n",
      "   treatment       mean   ci_lower  ci_upper       p_value\n",
      "0      tx_01   0.071836  -1.012446  1.156117  8.966841e-01\n",
      "1      tx_02  -0.120101  -1.103095  0.862894  8.107446e-01\n",
      "2      tx_03   0.074555  -0.937748  1.086857  8.852253e-01\n",
      "3      tx_04  -0.017847  -1.101717  1.066024  9.742551e-01\n",
      "4      tx_05   0.004221  -0.944001  0.952443  9.930394e-01\n",
      "5      tx_06  -0.044518  -1.038243  0.949207  9.300323e-01\n",
      "6      tx_07  -0.027821  -0.985599  0.929957  9.546000e-01\n",
      "7      tx_08  -0.831936  -1.867477  0.203606  1.153487e-01\n",
      "8      tx_09  -3.665079  -6.265183 -1.064974  5.731781e-03\n",
      "9      tx_10  -6.075323  -8.754146 -3.396501  8.788445e-06\n",
      "10     tx_11  -3.563576  -4.631572 -2.495580  6.160045e-11\n",
      "11     tx_12  -0.892695  -1.925715  0.140324  9.031762e-02\n",
      "12     tx_13   0.032783  -1.059739  1.125305  9.531022e-01\n",
      "13     tx_14  -0.023335  -1.044504  0.997835  9.642771e-01\n",
      "14     tx_15  -0.044766  -1.003899  0.914367  9.271124e-01\n",
      "15     tx_16   0.071303  -0.905367  1.047974  8.862189e-01\n",
      "16     tx_17   0.083880  -0.928026  1.095786  8.709375e-01\n",
      "17     tx_18  -0.054960  -1.245570  1.135650  9.279106e-01\n",
      "18     tx_19  -0.137462  -1.141444  0.866520  7.884284e-01\n",
      "19     tx_20   0.071951  -0.880446  1.024348  8.822871e-01\n",
      "20   tx_9_10 -12.301448 -14.768308 -9.834587  1.459961e-22\n"
     ]
    }
   ],
   "source": [
    "nsims = 600\n",
    "\n",
    "# [dml_function(x) for x in range(1, nsims + 1)]\n",
    "# dml_function(1)\n",
    "\n",
    "# ## all cw structures\n",
    "# for i in range(0, len(cw_combos)):\n",
    "for x in range(1, nsims + 1):\n",
    "    dml_function(x, cw_combos.loc[0, \"coefficients\"],\n",
    "                 cw_combos.loc[0, \"times\"],\n",
    "                 cw_combos.loc[0, \"sizes\"])\n",
    "        # print(x, cw_combos.loc[i, \"sizes\"], \n",
    "        #         cw_combos.loc[i, \"times\"])\n",
    "\n",
    "### missed one  naive narrow\n",
    "# for x in range(1, nsims + 1):\n",
    "#     dml_function(x, cw_combos.loc[1, \"coefficients\"], \n",
    "#                     cw_combos.loc[1, \"sizes\"], \n",
    "#                     cw_combos.loc[1, \"times\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test = est.marginal_ate_inference(T, X)\n",
    "# # test.shape()\n",
    "# # np.array(test.pvalue())\n",
    "# # test.pvalue()\n",
    "# # est.marginal_ate_inference(T, X)\n",
    "\n",
    "# # res.mean_point\n",
    "# norm_moderate_fx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# [item for item in norm_moderate_fx if item < -5]\n",
    "\n",
    "# fx = [\"T_\" + str(num) for num in \n",
    "#  [index for index, value in enumerate(norm_moderate_fx) if value < -5]]\n",
    "\n",
    "# # fx[0] * 2.5\n",
    "\n",
    "# # [item for item in T_vars if item in fx]\n",
    "# any(item in T_vars for item in fx)\n",
    "\n",
    "# # for i in fx:\n",
    "# #     print([item for item in T_vars if i in item])\n",
    "# # [item for item in my_list if search_string in item]\n",
    "    \n",
    "# T_vars[critical_to_confound] * 2.5\n",
    "# [sum(fx) for fx in zip(*T_vars[critical_to_confound]*2.5)] + np.random.normal(size = n_samples, loc = 0, scale = 1)\n",
    "# test = b_W0T9 * T_9 + b_W0T10 * T_10 + b_W0T11 * T_11 + np.random.normal(size=n_samples)\n",
    "# type(test)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
