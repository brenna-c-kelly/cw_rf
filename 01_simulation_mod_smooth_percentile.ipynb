{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulations\n",
    "\n",
    "This file includes the code for generating the simulated data. Each data file will contain a set of 5000 exposures randomly sampled without replacement from the Utah vital records cohort, the synthetic confounder, and the synthetic outcome. Indicator variables are also included to describe the critical window's structure.  \n",
    "\n",
    "To start, I'll create 100 simulated datasets with a moderate, smoothed critical window in the middle of the 20-week period."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Outside of loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "C extension: None not built. If you want to import pandas from the source directory, you may need to run 'python -m pip install -ve . --no-build-isolation -Ceditable-verbose=true' to build the C extensions first.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/__init__.py:22\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;66;03m# numpy compat\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     23\u001b[0m         is_numpy_dev \u001b[38;5;28;01mas\u001b[39;00m _is_numpy_dev,  \u001b[38;5;66;03m# pyright: ignore[reportUnusedImport] # noqa: F401\u001b[39;00m\n\u001b[1;32m     24\u001b[0m     )\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m _err:  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/compat/__init__.py:27\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_constants\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     19\u001b[0m     CHAINED_WARNING_DISABLED,\n\u001b[1;32m     20\u001b[0m     IS64,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     25\u001b[0m     WASM,\n\u001b[1;32m     26\u001b[0m )\n\u001b[0;32m---> 27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_numpy_dev\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyarrow\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     29\u001b[0m     HAS_PYARROW,\n\u001b[1;32m     30\u001b[0m     PYARROW_MIN_VERSION,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     38\u001b[0m     pa_version_under21p0,\n\u001b[1;32m     39\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/compat/numpy/__init__.py:18\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _nlv \u001b[38;5;241m<\u001b[39m Version(_min_numpy_ver):\n\u001b[0;32m---> 18\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m     19\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease upgrade numpy to >= \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_min_numpy_ver\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m to use this pandas version.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     20\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYour numpy version is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_np_version\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     21\u001b[0m     )\n\u001b[1;32m     24\u001b[0m np_long: \u001b[38;5;28mtype\u001b[39m\n",
      "\u001b[0;31mImportError\u001b[0m: Please upgrade numpy to >= 1.26.0 to use this pandas version.\nYour numpy version is 1.24.3.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstats\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mstats\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/__init__.py:27\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m _err:  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n\u001b[1;32m     26\u001b[0m     _module \u001b[38;5;241m=\u001b[39m _err\u001b[38;5;241m.\u001b[39mname\n\u001b[0;32m---> 27\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m     28\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC extension: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_module\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not built. If you want to import \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     29\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpandas from the source directory, you may need to run \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     30\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpython -m pip install -ve . --no-build-isolation -Ceditable-verbose=true\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     31\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto build the C extensions first.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     32\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m_err\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_config\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     35\u001b[0m     get_option,\n\u001b[1;32m     36\u001b[0m     set_option,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     40\u001b[0m     options,\n\u001b[1;32m     41\u001b[0m )\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# let init-time option registration happen\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: C extension: None not built. If you want to import pandas from the source directory, you may need to run 'python -m pip install -ve . --no-build-isolation -Ceditable-verbose=true' to build the C extensions first."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "import multiprocessing as mp\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats import norm\n",
    "import matplotlib.pyplot as plt\n",
    "from econml.dml import LinearDML\n",
    "from econml.dml import CausalForestDML\n",
    "\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Critical window coefficient creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'scipy.stats' has no attribute 'Uniform'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m window_center \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[1;32m      6\u001b[0m window_end \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m14\u001b[39m\n\u001b[0;32m----> 8\u001b[0m uni_pdf \u001b[38;5;241m=\u001b[39m stats\u001b[38;5;241m.\u001b[39mUniform(a \u001b[38;5;241m=\u001b[39m window_start, b \u001b[38;5;241m=\u001b[39m window_end)\n\u001b[1;32m      9\u001b[0m uni_wide_fx  \u001b[38;5;241m=\u001b[39m uni_pdf\u001b[38;5;241m.\u001b[39mpdf(study_period) \u001b[38;5;241m*\u001b[39m effect_size\n\u001b[1;32m     11\u001b[0m uni_pdf \u001b[38;5;241m=\u001b[39m stats\u001b[38;5;241m.\u001b[39mUniform(a \u001b[38;5;241m=\u001b[39m window_start \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, b \u001b[38;5;241m=\u001b[39m window_end \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'scipy.stats' has no attribute 'Uniform'"
     ]
    }
   ],
   "source": [
    "study_period = np.linspace(1, 20, 20)\n",
    "effect_size = -15 # total reduction of 15 percentiles (based on the average effect of a 200-gram reduction in birthweight, given Sun et al)\n",
    "# the window should be 9 wide, then 7 wide, then 3 wide\n",
    "window_start = 6\n",
    "window_center = 10\n",
    "window_end = 14\n",
    "\n",
    "uni_pdf = stats.Uniform(a = window_start, b = window_end)\n",
    "uni_wide_fx  = uni_pdf.pdf(study_period) * effect_size\n",
    "\n",
    "uni_pdf = stats.Uniform(a = window_start + 1, b = window_end - 1)\n",
    "uni_moderate_fx  = uni_pdf.pdf(study_period) * effect_size\n",
    "\n",
    "uni_pdf = stats.Uniform(a = window_start + 3, b = window_end - 3)\n",
    "uni_narrow_fx  = uni_pdf.pdf(study_period) * effect_size\n",
    "\n",
    "norm_pdf = stats.norm.pdf(study_period, window_center, 2.5)\n",
    "norm_wide_fx  = norm_pdf * effect_size \n",
    "\n",
    "norm_pdf = stats.norm.pdf(study_period, window_center, 1)\n",
    "norm_moderate_fx  = norm_pdf * effect_size \n",
    "\n",
    "norm_pdf = stats.norm.pdf(study_period, window_center, 0.5)\n",
    "norm_narrow_fx = norm_pdf * effect_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'uni_wide_fx' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43muni_wide_fx\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'uni_wide_fx' is not defined"
     ]
    }
   ],
   "source": [
    "uni_wide_fx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combinations to be simulated\n",
    "cw_coefs = pd.DataFrame({\"norm_wide_fx\": norm_wide_fx, \n",
    "              \"norm_moderate_fx\": norm_moderate_fx, \n",
    "              \"norm_narrow_fx\": norm_narrow_fx,\n",
    "              \"uni_wide_fx\": uni_wide_fx, \n",
    "              \"uni_moderate_fx\": uni_moderate_fx, \n",
    "              \"uni_narrow_fx\": uni_narrow_fx})\n",
    "\n",
    "cw_combos = pd.DataFrame({\"coefficients\": [cw_coefs[\"norm_wide_fx\"], \n",
    "                                           cw_coefs[\"norm_moderate_fx\"], \n",
    "                                           cw_coefs[\"norm_narrow_fx\"],\n",
    "                                           cw_coefs[\"uni_wide_fx\"], \n",
    "                                           cw_coefs[\"uni_moderate_fx\"], \n",
    "                                           cw_coefs[\"uni_narrow_fx\"]],\n",
    "                          \"sizes\": [\"wide\", \"moderate\", \"narrow\",\n",
    "                                    \"wide\", \"moderate\", \"narrow\"],\n",
    "                          \"times\": [\"smooth\", \"smooth\", \"smooth\",\n",
    "                                    \"naive\", \"naive\", \"naive\"]})\n",
    "\n",
    "# save, for plotting\n",
    "cw_coefs.to_csv(\"data/\" + \"true_cw_fx_\" + str(effect_size) + \".csv\", sep = ',', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Reading in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/75/b6cym0892z93rs6dbfy789240000gn/T/ipykernel_37036/2057111297.py:1: DtypeWarning: Columns (15,23) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  births = pd.read_csv(\"data/birth_w_percentile_confounders.csv\")\n"
     ]
    }
   ],
   "source": [
    "births = pd.read_csv(\"data/birth_w_percentile_confounders.csv\")\n",
    "births.head()\n",
    "\n",
    "o3_mean = births[['max_o3_01', 'max_o3_02', 'max_o3_03', 'max_o3_04', \n",
    "                  'max_o3_05', 'max_o3_06', 'max_o3_07', 'max_o3_08', \n",
    "                  'max_o3_09', 'max_o3_10', 'max_o3_11', 'max_o3_12', \n",
    "                  'max_o3_13', 'max_o3_14', 'max_o3_15', 'max_o3_16', \n",
    "                  'max_o3_17', 'max_o3_18', 'max_o3_19', 'max_o3_20']].values.mean()\n",
    "o3_sd = births[['max_o3_01', 'max_o3_02', 'max_o3_03', 'max_o3_04', \n",
    "                'max_o3_05', 'max_o3_06', 'max_o3_07', 'max_o3_08', \n",
    "                'max_o3_09', 'max_o3_10', 'max_o3_11', 'max_o3_12', \n",
    "                'max_o3_13', 'max_o3_14', 'max_o3_15', 'max_o3_16', \n",
    "                'max_o3_17', 'max_o3_18', 'max_o3_19', 'max_o3_20']].values.std()\n",
    "\n",
    "bwp_mean = births[\"bw_percentile\"].mean()\n",
    "bwp_sd = births[\"bw_percentile\"].std()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim_function(iteration, cw_window_type, window_size, window_time):\n",
    "        \n",
    "    ## collecting exposure data\n",
    "\n",
    "    n_samples = 5000\n",
    "\n",
    "    n_X = 1\n",
    "    n_T = 20\n",
    "    n_W = 1\n",
    "\n",
    "    # treatments / exposures; 10 ppb scale\n",
    "    T_sample = births.sample(n = n_samples, replace = False)\n",
    "\n",
    "    T_01 = (T_sample['max_o3_01'] - o3_mean) / 10\n",
    "    T_03 = (T_sample['max_o3_03'] - o3_mean) / 10\n",
    "    T_02 = (T_sample['max_o3_02'] - o3_mean) / 10\n",
    "    T_04 = (T_sample['max_o3_04'] - o3_mean) / 10\n",
    "    T_05 = (T_sample['max_o3_05'] - o3_mean) / 10\n",
    "    T_06 = (T_sample['max_o3_06'] - o3_mean) / 10\n",
    "    T_07 = (T_sample['max_o3_07'] - o3_mean) / 10\n",
    "    T_08 = (T_sample['max_o3_08'] - o3_mean) / 10\n",
    "    T_09 = (T_sample['max_o3_09'] - o3_mean) / 10\n",
    "    T_10 = (T_sample['max_o3_10'] - o3_mean) / 10\n",
    "    T_11 = (T_sample['max_o3_11'] - o3_mean) / 10\n",
    "    T_12 = (T_sample['max_o3_12'] - o3_mean) / 10\n",
    "    T_13 = (T_sample['max_o3_13'] - o3_mean) / 10\n",
    "    T_14 = (T_sample['max_o3_14'] - o3_mean) / 10\n",
    "    T_15 = (T_sample['max_o3_15'] - o3_mean) / 10\n",
    "    T_16 = (T_sample['max_o3_16'] - o3_mean) / 10\n",
    "    T_17 = (T_sample['max_o3_17'] - o3_mean) / 10\n",
    "    T_18 = (T_sample['max_o3_18'] - o3_mean) / 10\n",
    "    T_19 = (T_sample['max_o3_19'] - o3_mean) / 10\n",
    "    T_20 = (T_sample['max_o3_20'] - o3_mean) / 10\n",
    "\n",
    "    # stack\n",
    "    T_vars = np.vstack((T_01, T_02, T_03, T_04, T_05,\n",
    "                        T_06, T_07, T_08, T_09, T_10,\n",
    "                        T_11, T_12, T_13, T_14, T_15,\n",
    "                        T_16, T_17, T_18, T_19, T_20))\n",
    "\n",
    "    ## creating confounder coefficient\n",
    "\n",
    "    # we're only confounding the critical exposures if the effect is < -5 (can be changed later)\n",
    "    critical_to_confound = [index for index, value in enumerate(cw_window_type) if value < -0.5]\n",
    "\n",
    "    X = [sum(fx) for fx in zip(*T_vars[critical_to_confound]*1.25)] + np.random.normal(size = n_samples, loc = 0, scale = 1)\n",
    "    # z scale\n",
    "    X = X / X.std()\n",
    "    X = pd.Series(X)\n",
    "    X = X.to_numpy()\n",
    "\n",
    "    ## creating the outcome\n",
    "\n",
    "    # the coefficients and treatment variables\n",
    "    tx_fx, tx = cw_window_type, T_vars\n",
    "    # the vectors of each coefficient * treatment variable\n",
    "    tx_fx_list = [tx[i] * tx_fx[i] for i in np.arange(0, 20)]\n",
    "    # tx_fx_list = [tx[10] * tx_fx[10]]\n",
    "    # total treatment effect by individual\n",
    "    total_tx_fx = [sum(fx) for fx in zip(*tx_fx_list)]\n",
    "\n",
    "    # the confounder effect (z-scaled, see above)\n",
    "    b_W0y = 1.5\n",
    "    # the vector of confounder effects\n",
    "    wx_fx = X * b_W0y\n",
    "\n",
    "    # bit of noise\n",
    "    e = np.random.normal(size=n_samples, loc = 0, scale = 1)\n",
    "\n",
    "    b_int = bwp_mean\n",
    "\n",
    "    y = b_int + total_tx_fx + wx_fx + e\n",
    "\n",
    "    # adding an indicator for critical / not\n",
    "    # critical_ones = [index for index, value in enumerate(cw_window_type) if value < -10]\n",
    "\n",
    "    # critical_or_not = []\n",
    "\n",
    "    # for i in range(0, 20):\n",
    "    #     if i in critical_ones:\n",
    "    #         critical_or_not.append(\"critical\")\n",
    "    #     else:\n",
    "    #         critical_or_not.append(\"not critical\")\n",
    "\n",
    "    ## store the data file\n",
    "\n",
    "    sim_dat = pd.DataFrame({'sim_index': iteration, 'cw_size': window_size,\n",
    "                            'cw_timedep': window_time, #\"critical\": critical_or_not,\n",
    "                            'total_fx_size': effect_size,\n",
    "                            'y_hat': y, 'true_y': T_sample[\"birthweightgrams\"], 'x': X,\n",
    "                            'tx_01': T_01, 'tx_02': T_02, 'tx_03': T_03, 'tx_04': T_04, \n",
    "                            'tx_05': T_05, 'tx_06': T_06, 'tx_07': T_07, 'tx_08': T_08, \n",
    "                            'tx_09': T_09, 'tx_10': T_10, 'tx_11': T_11, 'tx_12': T_12, \n",
    "                            'tx_13': T_13, 'tx_14': T_14, 'tx_15': T_15, 'tx_16': T_16, \n",
    "                            'tx_17': T_17, 'tx_18': T_18, 'tx_19': T_19, 'tx_20': T_20})\n",
    "\n",
    "    sim_dat.to_csv(\"data/sims/\" + \"sim\" + str(iteration).zfill(3) + \"_\" + window_size + \"_\" + window_time + \".csv\", sep = ',', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nsims = 5\n",
    "\n",
    "# ## one cw structure\n",
    "# # [sim_function(x) for x in range(1, nsims+1)]\n",
    "\n",
    "# [sim_function(x, cw_combos.loc[1, \"coefficients\"], \n",
    "#         cw_combos.loc[1, \"sizes\"], \n",
    "#         cw_combos.loc[1, \"times\"]) for x in range(1, nsims+1)]\n",
    "\n",
    "\n",
    "# # with mp.Pool(mp.cpu_count()) as p:\n",
    "# #     p.map([sim_function(x, cw_combos.loc[1, \"coefficients\"], \n",
    "# #                         cw_combos.loc[1, \"sizes\"], \n",
    "# #                         cw_combos.loc[1, \"times\"]) for x in range(1, nsims+1)])\n",
    "# # ## all cw structures\n",
    "# # for i in range(0, len(cw_combos)-1):\n",
    "# #     [sim_function(x, cw_combos.loc[i, \"coefficients\"], \n",
    "# #              cw_combos.loc[i, \"sizes\"], \n",
    "# #              cw_combos.loc[i, \"times\"]) for x in range(1, nsims+1)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### QC\n",
    "Ran LinearDML to make sure one simulation gives expected result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T = pd.DataFrame({'noncritical_tx_1': T_01, 'noncritical_tx_2': T_02, \n",
    "#                   'noncritical_tx_3': T_03, 'noncritical_tx_4': T_04, \n",
    "#                   'noncritical_tx_5': T_05, 'noncritical_tx_6': T_06, \n",
    "#                   'noncritical_tx_7': T_07, 'critical_tx_8': T_08, \n",
    "#                   'critical_tx_9': T_09, 'critical_tx_10': T_10,\n",
    "#                   'critical_tx_11': T_11, 'critical_tx_12': T_12, \n",
    "#                   'noncritical_tx_13': T_13, 'noncritical_tx_14': T_14, \n",
    "#                   'noncritical_tx_15': T_15, 'noncritical_tx_16': T_16, \n",
    "#                   'noncritical_tx_17': T_17, 'noncritical_tx_18': T_18, \n",
    "#                   'noncritical_tx_19': T_19, 'noncritical_tx_20': T_20})\n",
    "# X = pd.DataFrame(X, columns=['confounder'])\n",
    "# y = pd.DataFrame({'birthweight': y})\n",
    "\n",
    "# model_y = 'linear'\n",
    "# model_t = 'linear'\n",
    "\n",
    "# # T = pd.DataFrame({\"tx_10\": tx[10]})\n",
    "\n",
    "# est = LinearDML(model_y=model_y, model_t=model_t,\n",
    "#                 discrete_treatment=False) \n",
    "\n",
    "# est.fit(y, T=T, W=X, X=X)\n",
    "\n",
    "# est.marginal_ate_inference(T, X)\n",
    "\n",
    "# ### seems to struggle with collinearity\n",
    "# ### - sometimes adjacent time steps are significant\n",
    "# ### - sometimes far time steps are significant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying simulated data to causal RF  \n",
    "\n",
    "This bit runs the causal RF with each simulated data file and stores the results.  \n",
    "Question: should we compare the causal random forest to the LinearDML?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Note: sim number is stored in the csv, in case we want additional comparisons later\n",
    "# sim_files = os.listdir(\"data/sims/\")\n",
    "\n",
    "# sim_csvs = []\n",
    "\n",
    "# for file_name in sim_files:\n",
    "#     if file_name.endswith('.csv'):\n",
    "#         sim_csvs.append(file_name)\n",
    "\n",
    "# sim_csvs.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dml_function(iteration, cw_window_type, window_size, window_time):\n",
    "\n",
    "    #### Part I: data generation\n",
    "    n_samples = 5000\n",
    "\n",
    "    n_X = 1\n",
    "    n_T = 20\n",
    "    n_W = 1\n",
    "\n",
    "    # treatments / exposures; 10 ppb scale\n",
    "    T_sample = births.sample(n = n_samples, replace = False)\n",
    "\n",
    "    T_01 = (T_sample['max_o3_01'] - o3_mean) / 10\n",
    "    T_03 = (T_sample['max_o3_03'] - o3_mean) / 10\n",
    "    T_02 = (T_sample['max_o3_02'] - o3_mean) / 10\n",
    "    T_04 = (T_sample['max_o3_04'] - o3_mean) / 10\n",
    "    T_05 = (T_sample['max_o3_05'] - o3_mean) / 10\n",
    "    T_06 = (T_sample['max_o3_06'] - o3_mean) / 10\n",
    "    T_07 = (T_sample['max_o3_07'] - o3_mean) / 10\n",
    "    T_08 = (T_sample['max_o3_08'] - o3_mean) / 10\n",
    "    T_09 = (T_sample['max_o3_09'] - o3_mean) / 10\n",
    "    T_10 = (T_sample['max_o3_10'] - o3_mean) / 10\n",
    "    T_11 = (T_sample['max_o3_11'] - o3_mean) / 10\n",
    "    T_12 = (T_sample['max_o3_12'] - o3_mean) / 10\n",
    "    T_13 = (T_sample['max_o3_13'] - o3_mean) / 10\n",
    "    T_14 = (T_sample['max_o3_14'] - o3_mean) / 10\n",
    "    T_15 = (T_sample['max_o3_15'] - o3_mean) / 10\n",
    "    T_16 = (T_sample['max_o3_16'] - o3_mean) / 10\n",
    "    T_17 = (T_sample['max_o3_17'] - o3_mean) / 10\n",
    "    T_18 = (T_sample['max_o3_18'] - o3_mean) / 10\n",
    "    T_19 = (T_sample['max_o3_19'] - o3_mean) / 10\n",
    "    T_20 = (T_sample['max_o3_20'] - o3_mean) / 10\n",
    "\n",
    "    # stack\n",
    "    T_vars = np.vstack((T_01, T_02, T_03, T_04, T_05,\n",
    "                        T_06, T_07, T_08, T_09, T_10,\n",
    "                        T_11, T_12, T_13, T_14, T_15,\n",
    "                        T_16, T_17, T_18, T_19, T_20))\n",
    "\n",
    "    ## creating confounder coefficient\n",
    "\n",
    "    # we're only confounding the critical exposures if the effect is < -5 (can be changed later)\n",
    "    critical_to_confound = [index for index, value in enumerate(cw_window_type) if value < -0.5]\n",
    "\n",
    "    X = [sum(fx) for fx in zip(*T_vars[critical_to_confound]*1.25)] + np.random.normal(size = n_samples, loc = 0, scale = 1)\n",
    "    # z scale\n",
    "    X = X / X.std()\n",
    "    X = pd.Series(X)\n",
    "    X = X.to_numpy()\n",
    "\n",
    "    ## creating the outcome\n",
    "\n",
    "    # the coefficients and treatment variables\n",
    "    tx_fx, tx = cw_window_type, T_vars\n",
    "    # the vectors of each coefficient * treatment variable\n",
    "    tx_fx_list = [tx[i] * tx_fx[i] for i in np.arange(0, 20)]\n",
    "    # tx_fx_list = [tx[10] * tx_fx[10]]\n",
    "    # total treatment effect by individual\n",
    "    total_tx_fx = [sum(fx) for fx in zip(*tx_fx_list)]\n",
    "\n",
    "    # the confounder effect (z-scaled, see above)\n",
    "    b_W0y = 1.5\n",
    "    # the vector of confounder effects\n",
    "    wx_fx = X * b_W0y\n",
    "\n",
    "    # bit of noise\n",
    "    e = np.random.normal(size=n_samples, loc = 0, scale = 1)\n",
    "\n",
    "    b_int = bwp_mean\n",
    "\n",
    "    y = b_int + total_tx_fx + wx_fx + e\n",
    "\n",
    "    sim_dat = pd.DataFrame({'sim_index': iteration, 'cw_size': window_size,\n",
    "                            'cw_timedep': window_time, #\"critical\": critical_or_not,\n",
    "                            'total_fx_size': effect_size,\n",
    "                            'y_hat': y, 'true_y': T_sample[\"birthweightgrams\"], 'x': X,\n",
    "                            'tx_01': T_01, 'tx_02': T_02, 'tx_03': T_03, 'tx_04': T_04, \n",
    "                            'tx_05': T_05, 'tx_06': T_06, 'tx_07': T_07, 'tx_08': T_08, \n",
    "                            'tx_09': T_09, 'tx_10': T_10, 'tx_11': T_11, 'tx_12': T_12, \n",
    "                            'tx_13': T_13, 'tx_14': T_14, 'tx_15': T_15, 'tx_16': T_16, \n",
    "                            'tx_17': T_17, 'tx_18': T_18, 'tx_19': T_19, 'tx_20': T_20})\n",
    "\n",
    "\n",
    "    #### Part II: model training\n",
    "    # dir = \"data/sims/\" + \"sim\" + str(iteration).zfill(3) + \"_\" + window_size + \"_\" + window_time + \".csv\"\n",
    "    # sim_dat = pd.read_csv(dir)\n",
    "\n",
    "    T = sim_dat.loc[:, \"tx_01\":\"tx_20\"]\n",
    "    X = pd.DataFrame({\"confounder\": sim_dat[\"x\"]})\n",
    "    y = pd.DataFrame({\"birthweight\": sim_dat[\"y_hat\"]})\n",
    "    # convert to 1d array\n",
    "    # y = y.to_numpy().flatten()\n",
    "\n",
    "    est = CausalForestDML(model_t='forest',\n",
    "                          model_y='forest',\n",
    "                          discrete_treatment=False,\n",
    "                          n_estimators=500,\n",
    "                          n_jobs = 4)\n",
    "\n",
    "    est.fit(y, T=T, X=X, W=X)\n",
    "\n",
    "    print(\"test1\")\n",
    "\n",
    "    # # extract results\n",
    "    treatments = np.array(est.cate_treatment_names())\n",
    "    res = est.marginal_ate_inference(T, X)\n",
    "    means = res.mean_point #est.marginal_ate(T, X)\n",
    "    ci_lower, ci_upper = res.conf_int_mean()\n",
    "    p_vals = res.pvalue()\n",
    "\n",
    "    print(\"test2\")\n",
    "\n",
    "    # # create dataframe\n",
    "    res_df = pd.DataFrame({\n",
    "        'sim_index': iteration,\n",
    "        'cw_size': window_size, 'cw_timedep': window_time,\n",
    "        'treatment': treatments,\n",
    "        'true_effect': cw_window_type,\n",
    "        'mean': means[0],\n",
    "        'ci_lower': ci_lower[0],\n",
    "        'ci_upper': ci_upper[0],\n",
    "        'p_value': p_vals[0]\n",
    "    })\n",
    "    \n",
    "    # indicate if critical\n",
    "    res_df[\"cw\"] = [\"critical\" if x < -0.5 else \"not critical\" for x in res_df[\"true_effect\"]]\n",
    "\n",
    "    ## test results for critical window\n",
    "\n",
    "    # 1. is the true effect recovered for each week?\n",
    "    res_df[\"effect_recovered\"] = res_df[\"true_effect\"].between(res_df[\"ci_lower\"], res_df[\"ci_upper\"])\n",
    "\n",
    "    # 2. did it get the trend? i.e., the difference in value over time\n",
    "    # what the difference should be:\n",
    "    diff_list_true = [np.nan]\n",
    "    for i in range(1, len(cw_window_type)):\n",
    "        diff_list_true.append(cw_window_type[i] - cw_window_type[i - 1])\n",
    "    res_df[\"trend_true\"] = diff_list_true\n",
    "\n",
    "    diff_list_pred = [np.nan]\n",
    "    for i in range(1, len(res_df[\"mean\"])):\n",
    "        diff_list_pred.append(res_df.loc[i, \"mean\"] - res_df.loc[i - 1, \"mean\"])\n",
    "    res_df[\"trend_pred\"] = diff_list_pred\n",
    "\n",
    "    # how close is the true trend to the observed trend?\n",
    "    res_df[\"trend_recovered_diff\"] = (res_df[\"trend_true\"] - res_df[\"trend_pred\"])\n",
    "\n",
    "    # 3. did it get the peak effect? i.e., the inflection point\n",
    "    # what the inflection point should be:\n",
    "    inflection = []\n",
    "    for i in range(len(diff_list_true) - 1):\n",
    "        if diff_list_true[i] < 0 and diff_list_true[i + 1] > 0:\n",
    "            inflection.append(\"peak\")\n",
    "        else:\n",
    "            inflection.append(\"not peak\")\n",
    "    inflection.append(np.nan) # no next value\n",
    "    res_df[\"inflection_true\"] = inflection\n",
    "\n",
    "    # what was the predicted inflection point?\n",
    "    inflection = []\n",
    "    for i in range(len(diff_list_pred) - 1):\n",
    "        if diff_list_pred[i] < 0 and diff_list_pred[i + 1] > 0:\n",
    "            inflection.append(\"peak\")\n",
    "        else:\n",
    "            inflection.append(\"not peak\")\n",
    "    inflection.append(np.nan) # no next value\n",
    "    res_df[\"inflection_pred\"] = inflection\n",
    "\n",
    "    # is the peak effect recovered?\n",
    "    res_df[\"inflection_recovered\"] = (res_df[\"inflection_true\"] == res_df[\"inflection_pred\"])\n",
    "\n",
    "    # 4. was the critical effect (>5) statistically significant at alpha = 0.05?\n",
    "    critical_sig = []\n",
    "\n",
    "    for index, item in enumerate(res_df[\"p_value\"]):\n",
    "        if index in [index for index, value in enumerate(cw_window_type) if value < -0.5]:\n",
    "            if item < 0.05:\n",
    "                critical_sig.append(\"TRUE\")\n",
    "            else:\n",
    "                critical_sig.append(\"FALSE\")\n",
    "        else:\n",
    "            critical_sig.append(np.nan)\n",
    "\n",
    "    res_df[\"critical_sig_recovered\"] = critical_sig\n",
    "\n",
    "    res_df.to_csv(\"data/sim_results/\" + \"sim\" + str(iteration).zfill(3) + \"_res\" + \"_\" + window_size + \"_\" + window_time + \".csv\", sep = ',', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dir = \"data/sims/\" + \"sim\" + str(1).zfill(3) + \"_\" + cw_combos.loc[0, \"sizes\"] + \"_\" + cw_combos.loc[0, \"times\"] + \".csv\"\n",
    "# sim_dat = pd.read_csv(dir)\n",
    "\n",
    "# T = sim_dat.loc[:, \"tx_01\":\"tx_20\"]\n",
    "# X = pd.DataFrame({\"confounder\": sim_dat[\"x\"]})\n",
    "# y = pd.DataFrame({\"birthweight\": sim_dat[\"y_hat\"]})\n",
    "\n",
    "# est = CausalForestDML(model_t='forest',\n",
    "#                         model_y='forest',\n",
    "#                         discrete_treatment=False,\n",
    "#                         n_estimators=500)\n",
    "\n",
    "# est.fit(y, T=T, X=X, W=X)\n",
    "\n",
    "# # # extract results\n",
    "# treatments = np.array(est.cate_treatment_names())\n",
    "# res = est.marginal_ate_inference(T, X)\n",
    "# means = res.mean_point #est.marginal_ate(T, X)\n",
    "# ci_lower, ci_upper = res.conf_int_mean()\n",
    "# p_vals = res.pvalue()\n",
    "\n",
    "# est.marginal_ate_inference(T, X)\n",
    "# norm_moderate_fx\n",
    "# cw_combos.loc[0, \"sizes\"]\n",
    "# cw_window_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/base.py:1389: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/base.py:1389: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/base.py:1389: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/base.py:1389: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/base.py:1389: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test1\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Number of features of the model must match the input. Model n_features is 20 and input n_features is 1 ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(cw_combos)):\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, nsims \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m----> 9\u001b[0m         \u001b[43mdml_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcw_combos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcoefficients\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mcw_combos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msizes\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mcw_combos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtimes\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m         \u001b[38;5;66;03m# print(x, cw_combos.loc[i, \"sizes\"], \u001b[39;00m\n\u001b[1;32m     13\u001b[0m         \u001b[38;5;66;03m#         cw_combos.loc[i, \"times\"])\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m#                     cw_combos.loc[1, \"sizes\"], \u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m#                     cw_combos.loc[1, \"times\"])\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[9], line 106\u001b[0m, in \u001b[0;36mdml_function\u001b[0;34m(iteration, cw_window_type, window_size, window_time)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;66;03m# # extract results\u001b[39;00m\n\u001b[1;32m    105\u001b[0m treatments \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(est\u001b[38;5;241m.\u001b[39mcate_treatment_names())\n\u001b[0;32m--> 106\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmarginal_ate_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m means \u001b[38;5;241m=\u001b[39m res\u001b[38;5;241m.\u001b[39mmean_point \u001b[38;5;66;03m#est.marginal_ate(T, X)\u001b[39;00m\n\u001b[1;32m    108\u001b[0m ci_lower, ci_upper \u001b[38;5;241m=\u001b[39m res\u001b[38;5;241m.\u001b[39mconf_int_mean()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/econml/_cate_estimator.py:341\u001b[0m, in \u001b[0;36mBaseCateEstimator._defer_to_inference.<locals>.call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    339\u001b[0m args \u001b[38;5;241m=\u001b[39m bound_args\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# remove self\u001b[39;00m\n\u001b[1;32m    340\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m bound_args\u001b[38;5;241m.\u001b[39mkwargs\n\u001b[0;32m--> 341\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_use_inference_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/econml/_cate_estimator.py:329\u001b[0m, in \u001b[0;36mBaseCateEstimator._use_inference_method\u001b[0;34m(self, name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_use_inference_method\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    328\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inference \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 329\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt call \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m because \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minference\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is None\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m name)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/econml/inference/_inference.py:46\u001b[0m, in \u001b[0;36mInference.marginal_ate_inference\u001b[0;34m(self, T, X)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmarginal_ate_inference\u001b[39m(\u001b[38;5;28mself\u001b[39m, T, X\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m---> 46\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmarginal_effect_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mpopulation_summary()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/econml/dml/causal_forest.py:209\u001b[0m, in \u001b[0;36m_GenericSingleOutcomeModelFinalWithCovInference.marginal_effect_inference\u001b[0;34m(self, T, X)\u001b[0m\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis inference method currently does not support X=None!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_est\u001b[38;5;241m.\u001b[39m_original_treatment_featurizer:\n\u001b[0;32m--> 209\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconst_marginal_effect_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    210\u001b[0m X, T \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_est\u001b[38;5;241m.\u001b[39m_expand_treatments(X, T, transform\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeaturizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/econml/dml/causal_forest.py:178\u001b[0m, in \u001b[0;36m_GenericSingleOutcomeModelFinalWithCovInference.const_marginal_effect_inference\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeaturizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    177\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeaturizer\u001b[38;5;241m.\u001b[39mtransform(X)\n\u001b[0;32m--> 178\u001b[0m pred, pred_var \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_final\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_and_var\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    179\u001b[0m pred \u001b[38;5;241m=\u001b[39m pred\u001b[38;5;241m.\u001b[39mreshape((\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_d_y \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_d_t)\n\u001b[1;32m    180\u001b[0m pred_stderr \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msqrt(np\u001b[38;5;241m.\u001b[39mdiagonal(pred_var, axis1\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, axis2\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\u001b[38;5;241m.\u001b[39mreshape((\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_d_y \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_d_t))\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/econml/grf/classes.py:51\u001b[0m, in \u001b[0;36mMultiOutputGRF.predict_and_var\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict_and_var\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m---> 51\u001b[0m     pred, var \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39m[\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_and_var\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m estimator \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_])\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mmoveaxis(np\u001b[38;5;241m.\u001b[39marray(pred), \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m), np\u001b[38;5;241m.\u001b[39mmoveaxis(np\u001b[38;5;241m.\u001b[39marray(var), \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/econml/grf/_base_grf.py:901\u001b[0m, in \u001b[0;36mBaseGRF.predict_and_var\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    884\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict_and_var\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[1;32m    885\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    886\u001b[0m \u001b[38;5;124;03m    Return the prefix of relevant fitted local parameters and their covariance matrix for each x in X.\u001b[39;00m\n\u001b[1;32m    887\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    899\u001b[0m \u001b[38;5;124;03m        The covariance of theta(x)[1, .., n_relevant_outputs]\u001b[39;00m\n\u001b[1;32m    900\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 901\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predict_point_and_var\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfull\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/econml/grf/_base_grf.py:706\u001b[0m, in \u001b[0;36mBaseGRF._predict_point_and_var\u001b[0;34m(self, X, full, point, var, project, projector)\u001b[0m\n\u001b[1;32m    665\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_predict_point_and_var\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, full\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, point\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, var\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, project\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, projector\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    666\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    667\u001b[0m \u001b[38;5;124;03m    Coordinate all prediction functionality.\u001b[39;00m\n\u001b[1;32m    668\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    704\u001b[0m \u001b[38;5;124;03m        then `x=n_relevant_outputs`.\u001b[39;00m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 706\u001b[0m     alpha, jac \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_alpha_and_jac\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    707\u001b[0m     invjac \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mpinv(jac)\n\u001b[1;32m    708\u001b[0m     parameter \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39meinsum(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mijk,ik->ij\u001b[39m\u001b[38;5;124m'\u001b[39m, invjac, alpha)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/econml/grf/_base_grf.py:640\u001b[0m, in \u001b[0;36mBaseGRF.predict_alpha_and_jac\u001b[0;34m(self, X, slice, parallel)\u001b[0m\n\u001b[1;32m    638\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    639\u001b[0m \u001b[38;5;66;03m# Check data\u001b[39;00m\n\u001b[0;32m--> 640\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_X_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    642\u001b[0m \u001b[38;5;66;03m# Assign chunk of trees to jobs\u001b[39;00m\n\u001b[1;32m    643\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mslice\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/econml/grf/_base_grf.py:472\u001b[0m, in \u001b[0;36mBaseGRF._validate_X_predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    469\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Validate X whenever one tries to predict, apply, and other predict methods.\"\"\"\u001b[39;00m\n\u001b[1;32m    470\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 472\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mestimators_\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_X_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/econml/tree/_tree_classes.py:288\u001b[0m, in \u001b[0;36mBaseTree._validate_X_predict\u001b[0;34m(self, X, check_input)\u001b[0m\n\u001b[1;32m    286\u001b[0m n_features \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_ \u001b[38;5;241m!=\u001b[39m n_features:\n\u001b[0;32m--> 288\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of features of the model must \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    289\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmatch the input. Model n_features is \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    290\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput n_features is \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    291\u001b[0m                      \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_, n_features))\n\u001b[1;32m    293\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X\n",
      "\u001b[0;31mValueError\u001b[0m: Number of features of the model must match the input. Model n_features is 20 and input n_features is 1 "
     ]
    }
   ],
   "source": [
    "nsims = 1\n",
    "\n",
    "# [dml_function(x) for x in range(1, nsims + 1)]\n",
    "# dml_function(1)\n",
    "\n",
    "# ## all cw structures\n",
    "for i in range(0, len(cw_combos)):\n",
    "    for x in range(1, nsims + 1):\n",
    "        dml_function(x, cw_combos.loc[i, \"coefficients\"],\n",
    "                     cw_combos.loc[i, \"sizes\"], \n",
    "                     cw_combos.loc[i, \"times\"])\n",
    "        # print(x, cw_combos.loc[i, \"sizes\"], \n",
    "        #         cw_combos.loc[i, \"times\"])\n",
    "\n",
    "### missed one  naive narrow\n",
    "# for x in range(1, nsims + 1):\n",
    "#     dml_function(x, cw_combos.loc[1, \"coefficients\"], \n",
    "#                     cw_combos.loc[1, \"sizes\"], \n",
    "#                     cw_combos.loc[1, \"times\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = est.marginal_ate_inference(T, X)\n",
    "# test.shape()\n",
    "# np.array(test.pvalue())\n",
    "# test.pvalue()\n",
    "# est.marginal_ate_inference(T, X)\n",
    "\n",
    "# res.mean_point\n",
    "norm_moderate_fx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "[item for item in norm_moderate_fx if item < -5]\n",
    "\n",
    "fx = [\"T_\" + str(num) for num in \n",
    " [index for index, value in enumerate(norm_moderate_fx) if value < -5]]\n",
    "\n",
    "# fx[0] * 2.5\n",
    "\n",
    "# [item for item in T_vars if item in fx]\n",
    "any(item in T_vars for item in fx)\n",
    "\n",
    "# for i in fx:\n",
    "#     print([item for item in T_vars if i in item])\n",
    "# [item for item in my_list if search_string in item]\n",
    "    \n",
    "T_vars[critical_to_confound] * 2.5\n",
    "[sum(fx) for fx in zip(*T_vars[critical_to_confound]*2.5)] + np.random.normal(size = n_samples, loc = 0, scale = 1)\n",
    "test = b_W0T9 * T_9 + b_W0T10 * T_10 + b_W0T11 * T_11 + np.random.normal(size=n_samples)\n",
    "type(test)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
