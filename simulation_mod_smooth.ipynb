{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulations\n",
    "\n",
    "This file includes the code for generating the simulated data. Each data file will contain a set of 5000 exposures randomly sampled without replacement from the Utah vital records cohort, the synthetic confounder, and the synthetic outcome. Indicator variables are also included to describe the critical window's structure.  \n",
    "\n",
    "To start, I'll create 100 simulated datasets with a moderate, smoothed critical window in the middle of the 20-week period."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Outside of loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats import norm\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from econml.dml import LinearDML\n",
    "from econml.dml import CausalForestDML\n",
    "\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Critical window coefficient creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study_period = np.linspace(1, 20, 20)\n",
    "effect_size = -166 # lower threshold: 166; middle: 200-517; upper: 647\n",
    "# the window should be 9 wide, then 7 wide, then 3 wide\n",
    "window_start = 6\n",
    "window_center = 10\n",
    "window_end = 14\n",
    "\n",
    "uni_pdf = stats.Uniform(a = window_start, b = window_end)\n",
    "uni_wide_fx  = uni_pdf.pdf(study_period) * effect_size\n",
    "\n",
    "uni_pdf = stats.Uniform(a = window_start + 1, b = window_end - 1)\n",
    "uni_moderate_fx  = uni_pdf.pdf(study_period) * effect_size\n",
    "\n",
    "uni_pdf = stats.Uniform(a = window_start + 3, b = window_end - 3)\n",
    "uni_narrow_fx  = uni_pdf.pdf(study_period) * effect_size\n",
    "\n",
    "norm_pdf = stats.norm.pdf(study_period, window_center, 2.5)\n",
    "norm_wide_fx  = norm_pdf * effect_size \n",
    "\n",
    "norm_pdf = stats.norm.pdf(study_period, window_center, 1)\n",
    "norm_moderate_fx  = norm_pdf * effect_size \n",
    "\n",
    "norm_pdf = stats.norm.pdf(study_period, window_center, 0.5)\n",
    "norm_narrow_fx = norm_pdf * effect_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combinations to be simulated\n",
    "cw_coefs = pd.DataFrame({\"norm_wide_fx\": norm_wide_fx, \n",
    "              \"norm_moderate_fx\": norm_moderate_fx, \n",
    "              \"norm_narrow_fx\": norm_narrow_fx,\n",
    "              \"uni_wide_fx\": uni_wide_fx, \n",
    "              \"uni_moderate_fx\": uni_moderate_fx, \n",
    "              \"uni_narrow_fx\": uni_narrow_fx})\n",
    "\n",
    "cw_combos = pd.DataFrame({\"coefficients\": [cw_coefs[\"norm_wide_fx\"], \n",
    "                                           cw_coefs[\"norm_moderate_fx\"], \n",
    "                                           cw_coefs[\"norm_narrow_fx\"],\n",
    "                                           cw_coefs[\"uni_wide_fx\"], \n",
    "                                           cw_coefs[\"uni_moderate_fx\"], \n",
    "                                           cw_coefs[\"uni_narrow_fx\"]],\n",
    "                          \"sizes\": [\"wide\", \"moderate\", \"narrow\",\n",
    "                                    \"wide\", \"moderate\", \"narrow\"],\n",
    "                          \"times\": [\"smooth\", \"smooth\", \"smooth\",\n",
    "                                    \"naive\", \"naive\", \"naive\"]})\n",
    "\n",
    "# save, for plotting\n",
    "cw_coefs.to_csv(\"data/\" + \"true_cw_fx_\" + str(effect_size) + \".csv\", sep = ',', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Reading in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "births = pd.read_csv(\"data/birth_clean_wide.csv\")\n",
    "births_long = pd.read_csv(\"data/birth_clean_long.csv\")\n",
    "\n",
    "o3_mean = births_long['max_o3'].mean()\n",
    "o3_sd = births_long['max_o3'].std()\n",
    "\n",
    "bw_mean = births[\"birthweightgrams\"].mean()\n",
    "bw_sd = births[\"birthweightgrams\"].std()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim_function(iteration, cw_window_type, window_size, window_time):\n",
    "        \n",
    "    ## collecting exposure data\n",
    "\n",
    "    n_samples = 5000 # 5000\n",
    "\n",
    "    n_X = 1 # one confounder\n",
    "    n_T = 20 # number of exposures\n",
    "    n_W = 1 # one confounder (must repeat)\n",
    "\n",
    "    # treatments / exposures; z-scaled\n",
    "    T_sample = births.sample(n = n_samples, replace = False)\n",
    "\n",
    "    T_1 = (T_sample['max_o3_01'] - o3_mean) / o3_sd\n",
    "    T_2 = (T_sample['max_o3_02'] - o3_mean) / o3_sd\n",
    "    T_3 = (T_sample['max_o3_03'] - o3_mean) / o3_sd\n",
    "    T_4 = (T_sample['max_o3_04'] - o3_mean) / o3_sd\n",
    "    T_5 = (T_sample['max_o3_05'] - o3_mean) / o3_sd\n",
    "    T_6 = (T_sample['max_o3_06'] - o3_mean) / o3_sd\n",
    "    T_7 = (T_sample['max_o3_07'] - o3_mean) / o3_sd\n",
    "    T_8 = (T_sample['max_o3_08'] - o3_mean) / o3_sd\n",
    "    T_9 = (T_sample['max_o3_09'] - o3_mean) / o3_sd\n",
    "    T_10 = (T_sample['max_o3_10'] - o3_mean) / o3_sd\n",
    "    T_11 = (T_sample['max_o3_11'] - o3_mean) / o3_sd\n",
    "    T_12 = (T_sample['max_o3_12'] - o3_mean) / o3_sd\n",
    "    T_13 = (T_sample['max_o3_13'] - o3_mean) / o3_sd\n",
    "    T_14 = (T_sample['max_o3_14'] - o3_mean) / o3_sd\n",
    "    T_15 = (T_sample['max_o3_15'] - o3_mean) / o3_sd\n",
    "    T_16 = (T_sample['max_o3_16'] - o3_mean) / o3_sd\n",
    "    T_17 = (T_sample['max_o3_17'] - o3_mean) / o3_sd\n",
    "    T_18 = (T_sample['max_o3_18'] - o3_mean) / o3_sd\n",
    "    T_19 = (T_sample['max_o3_19'] - o3_mean) / o3_sd\n",
    "    T_20 = (T_sample['max_o3_20'] - o3_mean) / o3_sd\n",
    "\n",
    "    # stack\n",
    "    T_vars = np.vstack((T_1, T_2, T_3, T_4, T_5,\n",
    "                        T_6, T_7, T_8, T_9, T_10,\n",
    "                        T_11, T_12, T_13, T_14, T_15,\n",
    "                        T_16, T_17, T_18, T_19, T_20))\n",
    "\n",
    "    ## creating confounder coefficient\n",
    "\n",
    "    # we're only confounding the critical exposures if the effect is < -5 (can be changed later)\n",
    "    critical_to_confound = [index for index, value in enumerate(cw_window_type) if value < -10]\n",
    "\n",
    "    X = [sum(fx) for fx in zip(*T_vars[critical_to_confound]*2.5)] + np.random.normal(size = n_samples, loc = 0, scale = 1)\n",
    "    # z scale\n",
    "    X = X / X.std()\n",
    "    X = pd.Series(X)\n",
    "    X = X.to_numpy()\n",
    "\n",
    "    ## creating the outcome\n",
    "\n",
    "    # the coefficients and treatment variables\n",
    "    tx_fx, tx = cw_window_type, T_vars\n",
    "    # the vectors of each coefficient * treatment variable\n",
    "    tx_fx_list = [tx[i] * tx_fx[i] for i in np.arange(0, 20)]\n",
    "    # tx_fx_list = [tx[10] * tx_fx[10]]\n",
    "    # total treatment effect by individual\n",
    "    total_tx_fx = [sum(fx) for fx in zip(*tx_fx_list)]\n",
    "\n",
    "    # the confounder effect (z-scaled, see above)\n",
    "    b_W0y = -50\n",
    "    # the vector of confounder effects\n",
    "    wx_fx = X * b_W0y\n",
    "\n",
    "    # bit of noise\n",
    "    e = np.random.normal(size=n_samples, loc = 0, scale = 10)\n",
    "\n",
    "    b_int = bw_mean\n",
    "\n",
    "    y = b_int + total_tx_fx + wx_fx + e\n",
    "\n",
    "    # adding an indicator for critical / not\n",
    "    # critical_ones = [index for index, value in enumerate(cw_window_type) if value < -10]\n",
    "\n",
    "    # critical_or_not = []\n",
    "\n",
    "    # for i in range(0, 20):\n",
    "    #     if i in critical_ones:\n",
    "    #         critical_or_not.append(\"critical\")\n",
    "    #     else:\n",
    "    #         critical_or_not.append(\"not critical\")\n",
    "\n",
    "    ## store the data file\n",
    "\n",
    "    sim_dat = pd.DataFrame({'sim_index': iteration, 'cw_size': window_size,\n",
    "                            'cw_timedep': window_time, #\"critical\": critical_or_not,\n",
    "                            'total_fx_size': effect_size,\n",
    "                            'y_hat': y, 'true_y': T_sample[\"birthweightgrams\"], 'x': X,\n",
    "                            'tx_01': T_1, 'tx_02': T_2, 'tx_03': T_3, 'tx_04': T_4, \n",
    "                            'tx_05': T_5, 'tx_06': T_6, 'tx_07': T_7, 'tx_08': T_8, \n",
    "                            'tx_09': T_9, 'tx_10': T_10, 'tx_11': T_11, 'tx_12': T_12, \n",
    "                            'tx_13': T_13, 'tx_14': T_14, 'tx_15': T_15, 'tx_16': T_16, \n",
    "                            'tx_17': T_17, 'tx_18': T_18, 'tx_19': T_19, 'tx_20': T_20})\n",
    "\n",
    "    sim_dat.to_csv(\"data/sims/\" + str(effect_size) + \"sim\" + str(iteration).zfill(3) + \"_\" + window_size + \"_\" + window_time + \".csv\", sep = ',', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nsims = 100\n",
    "\n",
    "## one cw structure\n",
    "# [sim_function(x) for x in range(1, nsims+1)]\n",
    "\n",
    "[sim_function(x, cw_combos.loc[5, \"coefficients\"], \n",
    "        cw_combos.loc[5, \"sizes\"], \n",
    "        cw_combos.loc[5, \"times\"]) for x in range(1, nsims+1)]\n",
    "\n",
    "# ## all cw structures\n",
    "# for i in range(0, len(cw_combos)-1):\n",
    "#     [sim_function(x, cw_combos.loc[i, \"coefficients\"], \n",
    "#              cw_combos.loc[i, \"sizes\"], \n",
    "#              cw_combos.loc[i, \"times\"]) for x in range(1, nsims+1)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### QC\n",
    "Ran LinearDML to make sure one simulation gives expected result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = pd.DataFrame({'noncritical_tx_1': T_1, 'noncritical_tx_2': T_2, \n",
    "                  'noncritical_tx_3': T_3, 'noncritical_tx_4': T_4, \n",
    "                  'noncritical_tx_5': T_5, 'noncritical_tx_6': T_6, \n",
    "                  'noncritical_tx_7': T_7, 'critical_tx_8': T_8, \n",
    "                  'critical_tx_9': T_9, 'critical_tx_10': T_10,\n",
    "                  'critical_tx_11': T_11, 'critical_tx_12': T_12, \n",
    "                  'noncritical_tx_13': T_13, 'noncritical_tx_14': T_14, \n",
    "                  'noncritical_tx_15': T_15, 'noncritical_tx_16': T_16, \n",
    "                  'noncritical_tx_17': T_17, 'noncritical_tx_18': T_18, \n",
    "                  'noncritical_tx_19': T_19, 'noncritical_tx_20': T_20})\n",
    "X = pd.DataFrame(X, columns=['confounder'])\n",
    "y = pd.DataFrame({'birthweight': y})\n",
    "\n",
    "model_y = 'linear'\n",
    "model_t = 'linear'\n",
    "\n",
    "# T = pd.DataFrame({\"tx_10\": tx[10]})\n",
    "\n",
    "est = LinearDML(model_y=model_y, model_t=model_t,\n",
    "                discrete_treatment=False) \n",
    "\n",
    "est.fit(y, T=T, W=X, X=X)\n",
    "\n",
    "est.marginal_ate_inference(T, X)\n",
    "\n",
    "### seems to struggle with collinearity\n",
    "### - sometimes adjacent time steps are significant\n",
    "### - sometimes far time steps are significant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying simulated data to causal RF  \n",
    "\n",
    "This bit runs the causal RF with each simulated data file and stores the results.  \n",
    "Question: should we compare the causal random forest to the LinearDML?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: sim number is stored in the csv, in case we want additional comparisons later\n",
    "sim_files = os.listdir(\"data/sims/\")\n",
    "\n",
    "sim_csvs = []\n",
    "\n",
    "for file_name in sim_files:\n",
    "    if file_name.endswith('.csv'):\n",
    "        sim_csvs.append(file_name)\n",
    "\n",
    "sim_csvs.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dml_function(iteration, cw_window_type, window_size, window_time):\n",
    "\n",
    "    # sim_dat = pd.read_csv(\"data/sims/\" + sim_csvs[iteration - 1])\n",
    "    dir = \"data/sims/\" + \"sim\" + str(iteration).zfill(3) + \"_\" + window_size + \"_\" + window_time + \".csv\"\n",
    "    sim_dat = pd.read_csv(dir)\n",
    "\n",
    "    T = sim_dat.loc[:, \"tx_01\":\"tx_20\"]\n",
    "    X = pd.DataFrame({\"confounder\": sim_dat[\"x\"]})\n",
    "    y = pd.DataFrame({\"birthweight\": sim_dat[\"y_hat\"]})\n",
    "\n",
    "    est = CausalForestDML(model_t='forest',\n",
    "                          model_y='forest',\n",
    "                          discrete_treatment=False,\n",
    "                          n_estimators=500)\n",
    "\n",
    "    est.fit(y, T=T, X=X, W=X)\n",
    "\n",
    "    # # extract results\n",
    "    treatments = np.array(est.cate_treatment_names())\n",
    "    res = est.marginal_ate_inference(T, X)\n",
    "    means = res.mean_point #est.marginal_ate(T, X)\n",
    "    ci_lower, ci_upper = res.conf_int_mean()\n",
    "    p_vals = res.pvalue()\n",
    "\n",
    "    # # create dataframe\n",
    "    res_df = pd.DataFrame({\n",
    "        'sim_index': sim_dat[\"sim_index\"][0],\n",
    "        'cw_size': window_size, 'cw_timedep': window_time,\n",
    "        'treatment': treatments,\n",
    "        'true_effect': cw_window_type,\n",
    "        'mean': means[0],\n",
    "        'ci_lower': ci_lower[0],\n",
    "        'ci_upper': ci_upper[0],\n",
    "        'p_value': p_vals[0]\n",
    "    })\n",
    "    \n",
    "    # indicate if critical\n",
    "    res_df[\"cw\"] = [\"critical\" if x < -10 else \"not critical\" for x in res_df[\"true_effect\"]]\n",
    "\n",
    "    ## test results for critical window\n",
    "\n",
    "    # 1. is the true effect recovered for each week?\n",
    "    res_df[\"effect_recovered\"] = res_df[\"true_effect\"].between(res_df[\"ci_lower\"], res_df[\"ci_upper\"])\n",
    "\n",
    "    # 2. did it get the trend? i.e., the difference in value over time\n",
    "    # what the difference should be:\n",
    "    diff_list_true = [np.nan]\n",
    "    for i in range(1, len(cw_window_type)):\n",
    "        diff_list_true.append(cw_window_type[i] - cw_window_type[i - 1])\n",
    "    res_df[\"trend_true\"] = diff_list_true\n",
    "\n",
    "    diff_list_pred = [np.nan]\n",
    "    for i in range(1, len(res_df[\"mean\"])):\n",
    "        diff_list_pred.append(res_df.loc[i, \"mean\"] - res_df.loc[i - 1, \"mean\"])\n",
    "    res_df[\"trend_pred\"] = diff_list_pred\n",
    "\n",
    "    # how close is the true trend to the observed trend?\n",
    "    res_df[\"trend_recovered_diff\"] = (res_df[\"trend_true\"] - res_df[\"trend_pred\"])\n",
    "\n",
    "    # 3. did it get the peak effect? i.e., the inflection point\n",
    "    # what the inflection point should be:\n",
    "    inflection = []\n",
    "    for i in range(len(diff_list_true) - 1):\n",
    "        if diff_list_true[i] < 0 and diff_list_true[i + 1] > 0:\n",
    "            inflection.append(\"peak\")\n",
    "        else:\n",
    "            inflection.append(\"not peak\")\n",
    "    inflection.append(np.nan) # no next value\n",
    "    res_df[\"inflection_true\"] = inflection\n",
    "\n",
    "    # what was the predicted inflection point?\n",
    "    inflection = []\n",
    "    print(len(diff_list_pred))\n",
    "    for i in range(len(diff_list_pred) - 1):\n",
    "        if diff_list_pred[i] < 0 and diff_list_pred[i + 1] > 0:\n",
    "            inflection.append(\"peak\")\n",
    "        else:\n",
    "            inflection.append(\"not peak\")\n",
    "    inflection.append(np.nan) # no next value\n",
    "    res_df[\"inflection_pred\"] = inflection\n",
    "\n",
    "    # is the peak effect recovered?\n",
    "    res_df[\"inflection_recovered\"] = (res_df[\"inflection_true\"] == res_df[\"inflection_pred\"])\n",
    "\n",
    "    # 4. was the critical effect (>5) statistically significant at alpha = 0.05?\n",
    "    critical_sig = []\n",
    "\n",
    "    for index, item in enumerate(res_df[\"p_value\"]):\n",
    "        if index in [index for index, value in enumerate(cw_window_type) if value < -10]:\n",
    "            if item < 0.05:\n",
    "                critical_sig.append(\"TRUE\")\n",
    "            else:\n",
    "                critical_sig.append(\"FALSE\")\n",
    "        else:\n",
    "            critical_sig.append(np.nan)\n",
    "\n",
    "    res_df[\"critical_sig_recovered\"] = critical_sig\n",
    "\n",
    "    res_df.to_csv(\"data/sim_results/\" + str(effect_size) + \"sim\" + str(sim_dat[\"sim_index\"][0]).zfill(3) + \"_res\" + \"_\" + window_size + \"_\" + window_time + \".csv\", sep = ',', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dir = \"data/sims/\" + \"sim\" + str(1).zfill(3) + \"_\" + cw_combos.loc[0, \"sizes\"] + \"_\" + cw_combos.loc[0, \"times\"] + \".csv\"\n",
    "# sim_dat = pd.read_csv(dir)\n",
    "\n",
    "# T = sim_dat.loc[:, \"tx_01\":\"tx_20\"]\n",
    "# X = pd.DataFrame({\"confounder\": sim_dat[\"x\"]})\n",
    "# y = pd.DataFrame({\"birthweight\": sim_dat[\"y_hat\"]})\n",
    "\n",
    "# est = CausalForestDML(model_t='forest',\n",
    "#                         model_y='forest',\n",
    "#                         discrete_treatment=False,\n",
    "#                         n_estimators=500)\n",
    "\n",
    "# est.fit(y, T=T, X=X, W=X)\n",
    "\n",
    "# # # extract results\n",
    "# treatments = np.array(est.cate_treatment_names())\n",
    "# res = est.marginal_ate_inference(T, X)\n",
    "# means = res.mean_point #est.marginal_ate(T, X)\n",
    "# ci_lower, ci_upper = res.conf_int_mean()\n",
    "# p_vals = res.pvalue()\n",
    "\n",
    "# est.marginal_ate_inference(T, X)\n",
    "norm_moderate_fx\n",
    "cw_combos.loc[0, \"sizes\"]\n",
    "cw_window_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nsims = 100\n",
    "\n",
    "# [dml_function(x) for x in range(1, nsims + 1)]\n",
    "# dml_function(1)\n",
    "\n",
    "# ## all cw structures\n",
    "# for i in range(2, len(cw_combos)):\n",
    "#     for x in range(1, nsims + 1):\n",
    "#         dml_function(x, cw_combos.loc[i, \"coefficients\"],\n",
    "#                      cw_combos.loc[i, \"sizes\"], \n",
    "#                      cw_combos.loc[i, \"times\"])\n",
    "#         # print(x, cw_combos.loc[i, \"sizes\"], \n",
    "#         #         cw_combos.loc[i, \"times\"])\n",
    "\n",
    "### missed one â€” naive narrow\n",
    "for x in range(1, nsims + 1):\n",
    "    dml_function(x, cw_combos.loc[5, \"coefficients\"],\n",
    "                    cw_combos.loc[5, \"sizes\"], \n",
    "                    cw_combos.loc[5, \"times\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = est.marginal_ate_inference(T, X)\n",
    "# test.shape()\n",
    "# np.array(test.pvalue())\n",
    "# test.pvalue()\n",
    "# est.marginal_ate_inference(T, X)\n",
    "\n",
    "# res.mean_point\n",
    "norm_moderate_fx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "[item for item in norm_moderate_fx if item < -5]\n",
    "\n",
    "fx = [\"T_\" + str(num) for num in \n",
    " [index for index, value in enumerate(norm_moderate_fx) if value < -5]]\n",
    "\n",
    "# fx[0] * 2.5\n",
    "\n",
    "# [item for item in T_vars if item in fx]\n",
    "any(item in T_vars for item in fx)\n",
    "\n",
    "# for i in fx:\n",
    "#     print([item for item in T_vars if i in item])\n",
    "# [item for item in my_list if search_string in item]\n",
    "    \n",
    "T_vars[critical_to_confound] * 2.5\n",
    "[sum(fx) for fx in zip(*T_vars[critical_to_confound]*2.5)] + np.random.normal(size = n_samples, loc = 0, scale = 1)\n",
    "test = b_W0T9 * T_9 + b_W0T10 * T_10 + b_W0T11 * T_11 + np.random.normal(size=n_samples)\n",
    "type(test)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
